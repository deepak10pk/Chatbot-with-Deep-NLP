{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building Chatbot.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpUbN1Vkkw8qS6wYlgljj7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepak10pk/Chatbot-with-Deep-NLP/blob/master/Building_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaxR6HuTkRbi",
        "colab_type": "text"
      },
      "source": [
        "## Install Tensorflow GPU for GPU usage "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soWK1nMWI0pv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "28f27090-d6ac-4e92-e54d-4fe9b9a9f799"
      },
      "source": [
        "!pip install tensorflow-gpu==1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/e2/7389e7a1c10eb209ddabe0b35cca2e522a3985a1df7e07904c76d1d5609c/tensorflow_gpu-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (95.3MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 58kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.18.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow-gpu==1.0) (46.1.3)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NaTJzFMJNnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kx-gNwjvz5",
        "colab_type": "text"
      },
      "source": [
        "## [Now we can remove the default cuda version because TF 1.0.0 needs 8.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrlepqUdJpPS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "487b26ee-9f21-4232-c438-85392cb48f3e"
      },
      "source": [
        "!apt-get remove cuda\n",
        "!apt-get autoremove cuda\n",
        "!apt-get purge cuda\n",
        "!apt-key del /var/cuda-repo-9-2-local/*.pub\n",
        "!rm -rf /var/cuda-repo-8-0-local-ga2/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C90GVKghKPC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ba3ce09-c1ac-4df8-dcb6-43b7a4048f99"
      },
      "source": [
        "!sudo wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-8-0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 04:54:03--  https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.16.29\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.16.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?FREF8rzgZ1k53Ks21dIdxJVqBgH-1lrQ9soNoPfdeKPr0L1ReKpeEaH_DzlzTvOyA3yv7a7XTDROOWemp7miPH7Lm7hl0g3cKeIPErLc5g-0hg68vPFyciO1T1xWVaNEHVcGmNA_x7tjO_7pztGcbPQk07i9zKHuxNI4knL9dF7s0MunBq1-GsxLkEy-mFZGoo2cMuy_-ASqugWE98SQCy1iDQ [following]\n",
            "--2020-04-28 04:54:04--  https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?FREF8rzgZ1k53Ks21dIdxJVqBgH-1lrQ9soNoPfdeKPr0L1ReKpeEaH_DzlzTvOyA3yv7a7XTDROOWemp7miPH7Lm7hl0g3cKeIPErLc5g-0hg68vPFyciO1T1xWVaNEHVcGmNA_x7tjO_7pztGcbPQk07i9zKHuxNI4knL9dF7s0MunBq1-GsxLkEy-mFZGoo2cMuy_-ASqugWE98SQCy1iDQ\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1913589814 (1.8G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.78G  45.7MB/s    in 14s     \n",
            "\n",
            "2020-04-28 04:54:18 (130 MB/s) - ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’ saved [1913589814/1913589814]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-8-0-local-ga2.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Warning: The postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2\n",
            "Warning: seems to use apt-key (provided by apt) without depending on gnupg or gnupg2.\n",
            "Warning: This will BREAK in the future and should be fixed by the package maintainer(s).\n",
            "Note: Check first if apt-key functionality is needed at all - it probably isn't!\n",
            "Warning: apt-key should not be used in scripts (called from postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2)\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-8-0-local-ga2  InRelease\n",
            "Ign:1 file:/var/cuda-repo-8-0-local-ga2  InRelease\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  Release.gpg [819 B]\n",
            "Get:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:11 file:/var/cuda-repo-8-0-local-ga2  Packages [22.7 kB]\n",
            "Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [37.4 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:16 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,811 kB]\n",
            "Get:18 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [874 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [44.6 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [8,213 B]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [839 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [889 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [59.0 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,184 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [12.6 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,372 kB]\n",
            "Get:27 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [89.3 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [7,671 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [8,286 B]\n",
            "Fetched 7,529 kB in 2s (4,858 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-8-0 cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "0 upgraded, 31 newly installed, 0 to remove and 97 not upgraded.\n",
            "Need to get 0 B/1,312 MB of archives.\n",
            "After this operation, 2,079 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-8-0-local-ga2  cuda-license-8-0 8.0.61-1 [27.6 kB]\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  cuda-misc-headers-8-0 8.0.61-1 [1,077 kB]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  cuda-core-8-0 8.0.61-1 [20.0 MB]\n",
            "Get:4 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-8-0 8.0.61-1 [135 kB]\n",
            "Get:5 file:/var/cuda-repo-8-0-local-ga2  cuda-driver-dev-8-0 8.0.61-1 [14.1 kB]\n",
            "Get:6 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-dev-8-0 8.0.61-1 [1,071 kB]\n",
            "Get:7 file:/var/cuda-repo-8-0-local-ga2  cuda-command-line-tools-8-0 8.0.61-1 [26.1 MB]\n",
            "Get:8 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-8-0 8.0.61-1 [9,585 kB]\n",
            "Get:9 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-dev-8-0 8.0.61-1 [10.8 kB]\n",
            "Get:10 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-8-0 8.0.61-1 [29.3 MB]\n",
            "Get:11 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-dev-8-0 8.0.61-1 [6,816 kB]\n",
            "Get:12 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-8-0 8.0.61-1 [27.2 MB]\n",
            "Get:13 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-dev-8-0 8.0.61-1 [57.4 MB]\n",
            "Get:14 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-8-0 8.0.61-1 [117 MB]\n",
            "Get:15 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-dev-8-0 8.0.61-1 [94.8 MB]\n",
            "Get:16 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-8-0 8.0.61-1 [43.7 MB]\n",
            "Get:17 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-dev-8-0 8.0.61-1 [67.7 MB]\n",
            "Get:18 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-8-0 8.0.61-1 [28.8 MB]\n",
            "Get:19 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-dev-8-0 8.0.61-1 [29.6 MB]\n",
            "Get:20 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-8-0 8.0.61-1 [157 MB]\n",
            "Get:21 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-dev-8-0 8.0.61-1 [82.3 MB]\n",
            "Get:22 file:/var/cuda-repo-8-0-local-ga2  cuda-samples-8-0 8.0.61-1 [101 MB]\n",
            "Get:23 file:/var/cuda-repo-8-0-local-ga2  cuda-documentation-8-0 8.0.61-1 [113 MB]\n",
            "Get:24 file:/var/cuda-repo-8-0-local-ga2  cuda-nvml-dev-8-0 8.0.61-1 [48.4 kB]\n",
            "Get:25 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-8-0 8.0.61-1 [2,948 kB]\n",
            "Get:26 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-dev-8-0 8.0.61-1 [3,028 kB]\n",
            "Get:27 file:/var/cuda-repo-8-0-local-ga2  cuda-visual-tools-8-0 8.0.61-1 [286 MB]\n",
            "Get:28 file:/var/cuda-repo-8-0-local-ga2  cuda-toolkit-8-0 8.0.61-1 [2,892 B]\n",
            "Get:29 file:/var/cuda-repo-8-0-local-ga2  cuda-runtime-8-0 8.0.61-1 [2,574 B]\n",
            "Get:30 file:/var/cuda-repo-8-0-local-ga2  cuda-demo-suite-8-0 8.0.61-1 [4,988 kB]\n",
            "Get:31 file:/var/cuda-repo-8-0-local-ga2  cuda-8-0 8.0.61-1 [2,556 B]\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 31.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cuda-license-8-0.\n",
            "(Reading database ... 144662 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-license-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-8-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-core-8-0.\n",
            "Preparing to unpack .../02-cuda-core-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-core-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-8-0.\n",
            "Preparing to unpack .../03-cuda-cudart-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-8-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-8-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-8-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-8-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-8-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-8-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-8-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-8-0.\n",
            "Preparing to unpack .../11-cuda-cublas-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-8-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-8-0.\n",
            "Preparing to unpack .../13-cuda-cufft-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-8-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-8-0.\n",
            "Preparing to unpack .../15-cuda-curand-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-8-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-8-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-8-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-8-0.\n",
            "Preparing to unpack .../19-cuda-npp-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-8-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-samples-8-0.\n",
            "Preparing to unpack .../21-cuda-samples-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-samples-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-documentation-8-0.\n",
            "Preparing to unpack .../22-cuda-documentation-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-8-0.\n",
            "Preparing to unpack .../23-cuda-nvml-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-8-0.\n",
            "Preparing to unpack .../24-cuda-nvgraph-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-8-0.\n",
            "Preparing to unpack .../25-cuda-nvgraph-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-8-0.\n",
            "Preparing to unpack .../26-cuda-visual-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-8-0.\n",
            "Preparing to unpack .../27-cuda-toolkit-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-runtime-8-0.\n",
            "Preparing to unpack .../28-cuda-runtime-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-8-0.\n",
            "Preparing to unpack .../29-cuda-demo-suite-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-8-0.\n",
            "Preparing to unpack .../30-cuda-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-license-8-0 (8.0.61-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-8.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-core-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-samples-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-8-0 (8.0.61-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRWcqYbXKhbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b5eb8d41-7011-4ec7-f61f-09068dfabc60"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2016 NVIDIA Corporation\n",
            "Built on Tue_Jan_10_13:22:03_CST_2017\n",
            "Cuda compilation tools, release 8.0, V8.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G36ffrWTLMBP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "58625b5c-e31a-4efc-96a3-b405122eea3e"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gpu:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbuhSAM-LP7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64e220b5-0159-4f92-eaec-5412d59716cb"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zulMxdniMSS-",
        "colab_type": "text"
      },
      "source": [
        "# Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfagncJnLTei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = open('/content/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('/content/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWcZ9p14RByz",
        "colab_type": "text"
      },
      "source": [
        "## creating a dictionary that maps each lines and its id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gEMDbiRMepx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2line={}\n",
        "for line in lines:\n",
        "  _line=line.split(' +++$+++ ')\n",
        "  if len(_line)==5:\n",
        "    id2line[_line[0]]=_line[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWiKwXCcQiVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c099aa1-4092-40b4-93ae-2faad4350168"
      },
      "source": [
        "id2line"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'L1045': 'They do not!',\n",
              " 'L1044': 'They do to!',\n",
              " 'L985': 'I hope so.',\n",
              " 'L984': 'She okay?',\n",
              " 'L925': \"Let's go.\",\n",
              " 'L924': 'Wow',\n",
              " 'L872': \"Okay -- you're gonna need to learn how to lie.\",\n",
              " 'L871': 'No',\n",
              " 'L870': 'I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n",
              " 'L869': 'Like my fear of wearing pastels?',\n",
              " 'L868': 'The \"real you\".',\n",
              " 'L867': 'What good stuff?',\n",
              " 'L866': \"I figured you'd get to the good stuff eventually.\",\n",
              " 'L865': 'Thank God!  If I had to hear one more story about your coiffure...',\n",
              " 'L864': \"Me.  This endless ...blonde babble. I'm like, boring myself.\",\n",
              " 'L863': 'What crap?',\n",
              " 'L862': 'do you listen to this crap?',\n",
              " 'L861': 'No...',\n",
              " 'L860': 'Then Guillermo says, \"If you go any lighter, you\\'re gonna look like an extra on 90210.\"',\n",
              " 'L699': 'You always been this selfish?',\n",
              " 'L698': 'But',\n",
              " 'L697': \"Then that's all you had to say.\",\n",
              " 'L696': 'Well, no...',\n",
              " 'L695': \"You never wanted to go out with 'me, did you?\",\n",
              " 'L694': 'I was?',\n",
              " 'L693': 'I looked for you back at the party, but you always seemed to be \"occupied\".',\n",
              " 'L663': 'Tons',\n",
              " 'L662': 'Have fun tonight?',\n",
              " 'L578': 'I believe we share an art instructor',\n",
              " 'L577': 'You know Chastity?',\n",
              " 'L576': 'Looks like things worked out tonight, huh?',\n",
              " 'L575': 'Hi.',\n",
              " 'L407': \"Who knows?  All I've ever heard her say is that she'd dip before dating a guy that smokes.\",\n",
              " 'L406': \"So that's the kind of guy she likes? Pretty ones?\",\n",
              " 'L405': \"Lesbian?  No. I found a picture of Jared Leto in one of her drawers, so I'm pretty sure she's not harboring same-sex tendencies.\",\n",
              " 'L404': \"She's not a...\",\n",
              " 'L403': \"I'm workin' on it. But she doesn't seem to be goin' for him.\",\n",
              " 'L402': \"I really, really, really wanna go, but I can't.  Not unless my sister goes.\",\n",
              " 'L401': 'Sure have.',\n",
              " 'L368': \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\",\n",
              " 'L367': 'How do you get your hair to look like that?',\n",
              " 'L366': \"You're sweet.\",\n",
              " 'L365': 'You have my word.  As a gentleman',\n",
              " 'L364': \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\",\n",
              " 'L363': 'You got something on your mind?',\n",
              " 'L281': 'Where?',\n",
              " 'L280': 'There.',\n",
              " 'L277': \"Well, there's someone I think might be --\",\n",
              " 'L276': 'How is our little Find the Wench A Date plan progressing?',\n",
              " 'L275': 'Forget French.',\n",
              " 'L274': \"That's because it's such a nice one.\",\n",
              " 'L273': \"I don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\",\n",
              " 'L272': \"Right.  See?  You're ready for the quiz.\",\n",
              " 'L271': \"C'esc ma tete. This is my head\",\n",
              " 'L208': 'Let me see what I can do.',\n",
              " 'L207': 'Gosh, if only we could find Kat a boyfriend...',\n",
              " 'L206': \"That's a shame.\",\n",
              " 'L205': 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
              " 'L204': 'Why?',\n",
              " 'L203': 'Seems like she could get a date easy enough...',\n",
              " 'L202': \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
              " 'L201': 'Cameron.',\n",
              " 'L200': \"No, no, it's my fault -- we didn't have a proper introduction ---\",\n",
              " 'L199': 'Forget it.',\n",
              " 'L198': \"You're asking me out.  That's so cute. What's your name again?\",\n",
              " 'L197': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\",\n",
              " 'L196': 'Not the hacking and gagging and spitting part.  Please.',\n",
              " 'L195': \"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              " 'L194': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
              " 'L953': 'I did.',\n",
              " 'L952': \"You think you ' re the only sophomore at the prom?\",\n",
              " 'L660': \"I don't have to be home 'til two.\",\n",
              " 'L659': 'I have to be home in twenty minutes.',\n",
              " 'L600': \"All I know is -- I'd give up my private line to go out with a guy like Joey.\",\n",
              " 'L599': \"Sometimes I wonder if the guys we're supposed to want to go out with are the ones we actually want to go out with, you know?\",\n",
              " 'L598': \"Bianca, I don't think the highlights of dating Joey Dorsey are going to include door-opening and coat-holding.\",\n",
              " 'L597': \"Combination.  I don't know -- I thought he'd be different.  More of a gentleman...\",\n",
              " 'L596': 'Is he oily or dry?',\n",
              " 'L595': \"He practically proposed when he found out we had the same dermatologist. I mean. Dr. Bonchowski is great an all, but he's not exactly relevant party conversation.\",\n",
              " 'L580': 'Would you mind getting me a drink, Cameron?',\n",
              " 'L579': 'Great',\n",
              " 'L573': 'Joey.',\n",
              " 'L572': 'Who?',\n",
              " 'L571': 'Where did he go?  He was just here.',\n",
              " 'L51': 'You might wanna think about it',\n",
              " 'L50': 'No.',\n",
              " 'L49': 'Did you change your hair?',\n",
              " 'L760': \"You know the deal.  I can ' t go if Kat doesn't go --\",\n",
              " 'L759': 'Listen, I want to talk to you about the prom.',\n",
              " 'L758': \"You're concentrating awfully hard considering it's gym class.\",\n",
              " 'L757': 'Hi, Joey.',\n",
              " 'L756': 'Hey, sweet cheeks.',\n",
              " 'L593': \"My agent says I've got a good shot at being the Prada guy next year.\",\n",
              " 'L592': 'Neat...',\n",
              " 'L591': \"It's a gay cruise line, but I'll be, like, wearing a uniform and stuff.\",\n",
              " 'L590': 'Queen Harry?',\n",
              " 'L589': 'So yeah, I\\'ve got the Sears catalog thing going -- and the tube sock gig \" that\\'s gonna be huge.  And then I\\'m up for an ad for Queen Harry next week.',\n",
              " 'L397': 'Hopefully.',\n",
              " 'L396': \"Exactly  So, you going to Bogey Lowenbrau's thing on Saturday?\",\n",
              " 'L395': 'Expensive?',\n",
              " 'L394': \"It's more\",\n",
              " 'L1052': 'Perm?',\n",
              " 'L1051': 'Patrick -- is that- a.',\n",
              " 'L1022': \"It's just you.\",\n",
              " 'L1021': 'Is that woman a complete fruit-loop or is it just me?',\n",
              " 'L1011': 'No! I just wanted',\n",
              " 'L1010': 'What? To completely damage me?  To send me to therapy forever? What?',\n",
              " 'L1009': 'I just wanted --',\n",
              " 'L1008': 'You set me up.',\n",
              " 'L1007': 'Let go!',\n",
              " 'L983': 'So did you',\n",
              " 'L982': 'You looked beautiful last night, you know.',\n",
              " 'L914': \"I guess I'll never know, will I?\",\n",
              " 'L913': \"Not all experiences are good, Bianca. You can't always trust the people you want to.\",\n",
              " 'L912': \"God, you're just like him! Just keep me locked away in the dark, so I can't experience anything for myself\",\n",
              " 'L911': 'I guess I thought I was protecting you.',\n",
              " 'L910': \"I'm not stupid enough to repeat your mistakes.\",\n",
              " 'L909': \"That's not\",\n",
              " 'L908': \"No. you didn't!  If you really thought I could make my own decisions, you would've let me go out with him instead of helping Daddy hold me hostage.\",\n",
              " 'L907': 'I wanted to let you make up your own mind about him.',\n",
              " 'L906': \"Why didn't you tell me?\",\n",
              " 'L905': 'After that, I swore I\\'d never do anything just because \"everyone else\" was doing it.  And I haven\\'t since. Except for Bogey\\'s party, and my stunning gastro-intestinal display --',\n",
              " 'L904': 'But',\n",
              " 'L903': \"Just once.  Afterwards, I told him I didn't want to anymore.  I wasn't ready. He got pissed.  Then he broke up with me.\",\n",
              " 'L902': 'You did what?',\n",
              " 'L901': 'He said everyone was doing it.  So I did it.',\n",
              " 'L900': 'As in...',\n",
              " 'L899': 'Now I do.  Back then, was a different story.',\n",
              " 'L898': 'But you hate Joey',\n",
              " 'L897': 'He was, like, a total babe',\n",
              " 'L896': 'Why?',\n",
              " 'L895': 'In 9th.  For a month',\n",
              " 'L894': 'What?',\n",
              " 'L893': 'Joey never told you we went out, did he?',\n",
              " 'L892': \"I wish I had that luxury. I'm the only sophomore that got asked to the prom and I can't go, because you won ' t.\",\n",
              " 'L891': \"I do care. But I'm a firm believer in doing something for your own reasons, not someone else ' s .\",\n",
              " 'L890': 'Like you care.',\n",
              " 'L889': \"Listen, I know you hate having to sit home because I'm not Susie High School.\",\n",
              " 'L656': \"You're welcome.\",\n",
              " 'L655': \"I don't get you.  You act like you're too good for any of this, and then you go totally apeshit when you get here.\",\n",
              " 'L602': \"I really don't think I need any social advice from you right now.\",\n",
              " 'L601': 'Bianca, I need to talk to you -- I need to tell you --',\n",
              " 'L543': 'Can we go now?',\n",
              " 'L542': 'You are so completely unbalanced.',\n",
              " 'L533': \"Yeah, he's your freak friend Mandella's boyfriend.  I guess since I'm not allowed to go out, I should obsess over a dead guy, too.\",\n",
              " 'L532': \"It's Shakespeare.  Maybe you've heard of him?\",\n",
              " 'L531': \"Like I'm supposed to know what that even means.\",\n",
              " 'L530': \"At least I'm not a clouted fen- sucked hedge-pig.\",\n",
              " 'L529': \"Can't you forget for just one night that you're completely wretched?\",\n",
              " 'L527': \"Bogey Lowenstein's party is normal, but you're too busy listening to Bitches Who Need Prozac to know that.\",\n",
              " 'L526': \"What's normal?\",\n",
              " 'L525': \"You're ruining my life'  Because you won't be normal, I can't be normal.\",\n",
              " 'L445': \"I think you're a freak.  I think you do this to torture me.  And I think you suck.\",\n",
              " 'L444': 'What do you think?',\n",
              " 'L443': \"Oh, I thought you might have a date  I don't know why I'm bothering to ask, but are you going to Bogey Lowenstein's party Saturday night?\",\n",
              " 'L442': \"It means that Gigglepuss is playing at Club Skunk and we're going.\",\n",
              " 'L441': \"Oh my God, does this mean you're becoming normal?\",\n",
              " 'L320': 'Can you at least start wearing a bra?',\n",
              " 'L319': \"I have the potential to smack the crap out of you if you don't get out of my way.\",\n",
              " 'L165': 'Nowhere... Hi, Daddy.',\n",
              " 'L164': \"Where've you been?\",\n",
              " 'L923': \"I have a date, Daddy.  And he ' s not a captain of oppression like some men we know.\",\n",
              " 'L922': \"I'm missing something.\",\n",
              " 'L884': \"Fine.  I see that I'm a prisoner in my own house.  I'm not a daughter. I'm a possession!\",\n",
              " 'L883': \"You're not going unless your sister goes.  End of story.\",\n",
              " 'L882': 'He\\'s not a \"hot rod\".  Whatever that is.',\n",
              " 'L881': \"It's that hot rod Joey, right? That ' s who you want me to bend my rules for?\",\n",
              " 'L880': 'No, but',\n",
              " 'L879': 'The prom?  Kat has a date?',\n",
              " 'L878': \"Daddy, I want to discuss the prom with you. It's tomorrow night --\",\n",
              " 'L546': \"Because she'll scare them away.\",\n",
              " 'L545': 'Why?',\n",
              " 'L544': \"Promise me you won't talk to any boys unless your sister is present.\",\n",
              " 'L540': 'Just for a minute',\n",
              " 'L539': 'Daddy, no!',\n",
              " 'L538': 'Wear the belly before you go.',\n",
              " 'L537': \"It's just a party. Daddy.\",\n",
              " 'L536': \"Oh, God.  It's starting.\",\n",
              " 'L524': \"If Kat's not going, you're not going.\",\n",
              " 'L523': 'Daddy, people expect me to be there!',\n",
              " 'L521': 'It\\'s just a party. Daddy, but I knew you\\'d forbid me to go since \"Gloria Steinem\" over there isn\\'t going --',\n",
              " 'L520': 'Otherwise known as an orgy?',\n",
              " 'L519': 'If you must know, we were attempting to go to a small study group of friends.',\n",
              " 'L518': \"And where're you going?\",\n",
              " 'L517': 'Daddy, I --',\n",
              " 'L190': 'Exactly my point',\n",
              " 'L189': \"But she doesn't want to date.\",\n",
              " 'L183': \"But it's not fair -- she's a mutant, Daddy!\",\n",
              " 'L182': \"Then neither will you.  And I'll get to sleep at night.\",\n",
              " 'L181': 'What if she never starts dating?',\n",
              " 'L180': \"No! You're not dating until your sister starts dating.  End of discussion.\",\n",
              " 'L179': \"Now don't get upset. Daddy, but there's this boy... and I think he might ask...\",\n",
              " 'L463': \"Just sent 'em through.\",\n",
              " 'L462': 'Padua girls.  One tall, decent body. The other one kinda short and undersexed?',\n",
              " 'L461': 'Never',\n",
              " 'L460': 'Fan of a fan.  You see a couple of minors come in?',\n",
              " 'L459': \"Didn't have you pegged for a Gigglepuss fan.  Aren't they a little too pre-teen belly-button ring for you?\",\n",
              " 'L458': 'Always a pleasure, Brucie.',\n",
              " 'L779': \"Best case scenario, you're back on the payroll for awhile.\",\n",
              " 'L778': 'You humiliated the woman! Sacrifice yourself on the altar of dignity and even the score.',\n",
              " 'L586': \"No, I ' m not.\",\n",
              " 'L585': \"Buttholus extremus.  But hey, you're making progress.\",\n",
              " 'L584': \"The hell is that?  What kind of 'guy just picks up a girl and carries her away while you're talking to her?\",\n",
              " 'L583': 'Extremely unfortunate maneuver.',\n",
              " 'L500': \"Hell, I've just been going over the whole thing in my head and -\",\n",
              " 'L499': 'You told me that part already.',\n",
              " 'L433': \"It's her favorite band.\",\n",
              " 'L432': 'Assail your ears for one night.',\n",
              " 'L425': \"Okay!  I wasn't sure\",\n",
              " 'L424': \"He's pretty!\",\n",
              " 'L419': 'Dead at forty-one.',\n",
              " 'L418': 'Her favorite uncle',\n",
              " 'L417': \"It's a lung cancer issue\",\n",
              " 'L416': 'Number one.  She hates smokers',\n",
              " 'L391': \"Are you kidding?  He'll piss himself with joy.  He's the ultimate kiss ass.\",\n",
              " 'L390': 'Will Bogey get bent?',\n",
              " 'L389': \"In that case, we'll need to make it a school-wide blow out.\",\n",
              " 'L388': 'This is it.  A golden opportunity. Patrick can ask Katarina to the party.',\n",
              " 'L245': \"Like we had a choice?  Besides -- when you let the enemy think he's orchestrating the battle, you're in a position of power. We let him pretend he's calling the shots, and while he's busy setting up the plan, you have time to woo Bianca.\",\n",
              " 'L244': 'You got him involved?',\n",
              " 'L225': \"Hey -- I've gotta have a few clients when I get to Wall Street.\",\n",
              " 'L224': 'I thought you hated those people.',\n",
              " 'L223': \"You know, if you do go out with Bianca, you'd be set.  You'd outrank everyone. Strictly A-list.  With me by your side.\",\n",
              " 'L222': \"That's what I just said\",\n",
              " 'L221': \"Did she actually say she'd go out with you?\",\n",
              " 'L220': \"Forget his reputation.  Do you think we've got a plan or not?\",\n",
              " 'L219': \"I'm serious, man, he's whacked.  He sold his own liver on the black market so he could buy new speakers.\",\n",
              " 'L218': 'They always let felons sit in on Honors Biology?',\n",
              " 'L217': \"No kidding.  He's a criminal.  I heard he lit a state trooper on fire.  He just got out of Alcatraz...\",\n",
              " 'L216': 'He seems like he thrives on danger',\n",
              " 'L215': \"What makes you think he'll do it?\",\n",
              " 'L213': 'You wanna go out with him?',\n",
              " 'L212': 'What about him?',\n",
              " 'L211': \"Unlikely, but even so, she still can't go out with you.  So what's the point?\",\n",
              " 'L210': 'I teach her French, get to know her, dazzle her with charm and she falls in love with me.',\n",
              " 'L159': 'The mewling, rampalian wretch herself.',\n",
              " 'L158': \"That's her?  Bianca's sister?\",\n",
              " 'L157': 'Yeah, just a minor encounter with the shrew.',\n",
              " 'L147': 'You could consecrate with her, my friend.',\n",
              " 'L146': \"You mean I'd get a chance to talk to her?\",\n",
              " 'L145': 'Guess who just signed up for a tutor?',\n",
              " 'L144': \"Sure do ... my Mom's from Canada\",\n",
              " 'L143': 'You know French?',\n",
              " 'L142': \"Joey Dorsey?  Perma-shit-grin.  I wish I could say he's a moron, but he's number twelve in the class.  And a model.  Mostly regional stuff, but he's rumored to have a big tube sock ad coming out.\",\n",
              " 'L141': 'He always have that shit-eating grin?',\n",
              " 'L140': \"Because they're bred to.  Their mothers liked guys like that, and their grandmothers before them. Their gene pool is rarely diluted.\",\n",
              " 'L139': 'Why do girls like that always like guys like that?',\n",
              " 'L92': \"I could start with your haircut, but it doesn't matter.  She's not allowed to date until her older sister does.  And that's an impossibility.\",\n",
              " 'L91': 'Why not?',\n",
              " 'L90': \"Bianca Stratford.  Sophomore. Don't even think about it\",\n",
              " 'L89': 'Who is she?',\n",
              " 'L88': 'You burn, you pine, you perish?',\n",
              " 'L87': 'That girl -- I --',\n",
              " 'L78': 'Yeah, but these guys have never seen a horse.  They just jack off to Clint Eastwood.',\n",
              " 'L77': \"That I'm used to.\",\n",
              " 'L74': 'Couple thousand. Most of them evil',\n",
              " 'L73': 'How many people go here?',\n",
              " 'L72': 'Get out!',\n",
              " 'L71': 'Thirty-two.',\n",
              " 'L70': 'How many people were in your old school?',\n",
              " 'L69': \"Yeah.  A couple.  We're outnumbered by the cows, though.\",\n",
              " 'L68': 'I was kidding. People actually live there?',\n",
              " 'L67': \"North, actually.  How'd you   ?\",\n",
              " 'L66': 'So -- which Dakota you from?',\n",
              " 'L65': \"C'mon.  I'm supposed to give you the tour.\",\n",
              " 'L64': 'So they tell me...',\n",
              " 'L63': 'You the new guy?',\n",
              " 'L781': 'You get the girl.',\n",
              " 'L780': \"What's the worst?\",\n",
              " 'L745': 'Where?',\n",
              " 'L744': 'She kissed me.',\n",
              " 'L743': \"You makin' any headway?\",\n",
              " 'L741': \"She just needs time to cool off I'll give it a day.\",\n",
              " 'L740': \"She hates you with the fire of a thousand suns .  That's a direct quote\",\n",
              " 'L726': \"I don ' t know.  I decided not to nail her when she was too drunk to remember it.\",\n",
              " 'L725': \"What'd you do to her?\",\n",
              " 'L625': 'Then, go get her',\n",
              " 'L624': 'Sure',\n",
              " 'L623': 'Cameron -- do you like the girl?',\n",
              " 'L622': \"She's partial to Joey, not me\",\n",
              " 'L621': \"What 're you talking about?\",\n",
              " 'L620': \"It's off. The whole thing.\",\n",
              " 'L619': \"Cameron, I'm a little busy\",\n",
              " 'L431': \"Don't make me do it, man\",\n",
              " 'L430': 'Gigglepuss is playing there tomorrow night.',\n",
              " 'L427': \"So what does that give me?  I'm supposed to buy her some noodles and a book and sit around listening to chicks who can't play their instruments?\",\n",
              " 'L426': 'Okay -- Likes:  Thai food, feminist prose, and \"angry, stinky girl music of the indie-rock persuasion\".',\n",
              " 'L412': \"I've retrieved certain pieces of information on Miss Katarina Stratford I think you'll find helpful.\",\n",
              " 'L411': \"What've you got for me?\",\n",
              " 'L386': \"Yeah -- we'll see.\",\n",
              " 'L385': 'And he means that strictly in a non- prison-movie type of way.',\n",
              " 'L558': 'And why would I do that?',\n",
              " 'L557': 'Leave my sister alone.',\n",
              " 'L556': 'Your sister here?',\n",
              " 'L555': 'Away.',\n",
              " 'L554': 'Where ya goin?',\n",
              " 'L336': 'Not at all',\n",
              " 'L335': 'Hey -- do you mind?',\n",
              " 'L150': \"They're running the rest of me next month.\",\n",
              " 'L149': 'Yeah, and I noticed the only part of you featured in your big Kmart spread was your elbow.  Tough break.',\n",
              " 'L148': \"The vintage look is over, Kat. Haven't you been reading your Sassy?\",\n",
              " 'L748': \"Enough with the Barbie n' Ken shit. I know.\",\n",
              " 'L747': \"I don't know, Dorsey. ..the limo.-the flowers.  Another hundred for the tux --\",\n",
              " 'L609': 'Get her to act like a human',\n",
              " 'L608': 'Do what?',\n",
              " 'L607': \"How'd you do it?\",\n",
              " 'L606': \"A deal's a deal.\",\n",
              " 'L605': \"It's about time.\",\n",
              " 'L358': 'Forget her sister, then.',\n",
              " 'L357': 'Forget it.',\n",
              " 'L356': 'A hundred bucks a date.',\n",
              " 'L355': 'What?',\n",
              " 'L354': 'I just upped my price',\n",
              " 'L352': 'I got her under control. She just acts crazed in public to keep up the image.',\n",
              " 'L351': \"Watching the bitch trash my car doesn't count as a date.\",\n",
              " 'L350': \"I'm on it\",\n",
              " 'L349': 'When I shell out fifty, I expect results.',\n",
              " 'L301': \"Fifty, and you've got your man.\",\n",
              " 'L300': \"Take it or leave it.  This isn't a negotiation.\",\n",
              " 'L299': 'Fine, thirty.',\n",
              " 'L298': \"I can't take a girl like that out on twenty bucks.\",\n",
              " 'L297': 'How much?',\n",
              " 'L296': \"I can't date her sister until that one gets a boyfriend.  And that's the catch. She doesn't want a boyfriend.\",\n",
              " 'L295': \"You're gonna pay me to take out some girl?\",\n",
              " 'L294': 'You got it, Verona.  I pick up the tab, you do the honors.',\n",
              " 'L292': \"But you'd go out with her if you had the cake?\",\n",
              " 'L291': 'You need money to take a girl out',\n",
              " 'L290': 'You just said',\n",
              " 'L289': \"Sure, Sparky.  I'll get right on it.\",\n",
              " 'L288': 'Yeah, whatever.  I want you to go out with her.',\n",
              " 'L287': 'Two legs, nice rack...',\n",
              " 'L286': 'What do you think?',\n",
              " 'L285': 'Yeah',\n",
              " 'L509': \"Hey -- it's all for the higher good right?\",\n",
              " 'L508': \"You better not fuck this up.  I'm heavily invested.\",\n",
              " 'L505': 'What?  We took bathes together when we were kids.',\n",
              " 'L504': 'You and Verona?',\n",
              " 'L503': \"Uh,  yeah.  We're old friend*\",\n",
              " 'L502': \"I hear you're helpin' Verona.\",\n",
              " 'L240': \"So what you need to do is recruit a guy who'll go out with her.  Someone who's up for the job.\",\n",
              " 'L239': 'Does this conversation have a purpose?',\n",
              " 'L238': \"But she can't go out with you because her sister is this insane head case and no one will go out with her. right?\",\n",
              " 'L236': \"We're not.\",\n",
              " 'L235': \"Well, actually, I thought I'd run an idea by you.  You know, just to see if you're interested.\",\n",
              " 'L234': \"We don't chat.\",\n",
              " 'L233': 'Nope - just came by to chat',\n",
              " 'L232': 'Are you lost?',\n",
              " 'L231': 'Hey.',\n",
              " 'L949': \"Oh, honey -- tell me we haven't' progressed to full-on hallucinations.\",\n",
              " 'L948': 'William - he asked me to meet him here.',\n",
              " 'L947': 'Who?',\n",
              " 'L946': 'Have you seen him?',\n",
              " 'L755': 'Oh, good.  Something new and different for us.',\n",
              " 'L754': \"You ' re looking at this from the wrong perspective.  We're making a statement.\",\n",
              " 'L753': \"Okay, okay, we won't go.  It's not like I have a dress anyway\",\n",
              " 'L752': 'Listen to you!  You sound like Betty, all pissed off because Archie is taking Veronica.',\n",
              " 'L751': \"Well, I guess we're not, since we don't have dates .\",\n",
              " 'L750': 'Can you even imagine?  Who the hell would go to this a bastion of commercial excess?',\n",
              " 'L721': 'I got drunk.  I puked.  I got rejected. It was big fun.',\n",
              " 'L720': \"You didn't\",\n",
              " 'L719': 'I did Bianca a favor and it backfired.',\n",
              " 'L718': \"You didn't have a choice?  Where's Kat and what have you done with her?\",\n",
              " 'L717': \"I didn't have a choice.\",\n",
              " 'L716': 'You went to the party?  I thought we were officially opposed to suburban social activity.',\n",
              " 'L491': 'Who cares?',\n",
              " 'L490': \"What'd he say?\",\n",
              " 'L448': 'No fear.',\n",
              " 'L447': \"You think this'll work?\",\n",
              " 'L254': 'If I was Bianca, it would be, \"Any school you want, precious.  Don\\'t forget your tiara.\"',\n",
              " 'L253': 'Does it matter?',\n",
              " 'L252': \"I appreciate your efforts toward a speedy death, but I'm consuming.  Do you mind?\",\n",
              " 'L251': 'Neither has his heterosexuality.',\n",
              " 'L250': \"That's never been proven\",\n",
              " 'L249': \"William didn't even go to high school\",\n",
              " 'L248': 'William would never have gone to a state school.',\n",
              " 'L247': 'So he has this huge raging fit about Sarah Lawrence and insists that I go to his male-dominated, puking frat boy, number one golf team school. I have no say at all.',\n",
              " 'L152': \"You could always go with me.  I'm sure William has some friends.\",\n",
              " 'L151': 'The people at this school are so incredibly foul.',\n",
              " 'L134': \"But imagine the things he'd say during sex.\",\n",
              " 'L133': \"I realize that the men of this fine institution are severely lacking, but killing yourself so you can be with William Shakespeare is beyond the scope of normal teenage obsessions.  You're venturing far past daytime talk show fodder and entering the world of those who need very expensive therapy.\",\n",
              " 'L132': 'An attempted slit.',\n",
              " 'L131': \"What's this?\",\n",
              " 'L130': 'Just a little.',\n",
              " 'L129': 'Mandella, eat.  Starving yourself is a very slow way to die.',\n",
              " 'L128': 'Block E?',\n",
              " 'L127': 'He always look so',\n",
              " 'L126': \"I'm sure he's completely incapable of doing anything that interesting.\",\n",
              " 'L125': \"That's Pat Verona? The one who was gone for a year? I heard he was doing porn movies.\",\n",
              " 'L124': 'Patrick Verona   Random skid.',\n",
              " 'L123': \"Who's that?\",\n",
              " 'L1043': \"Don ' t you even dare. . .\",\n",
              " 'L1042': 'Oh, Bianca?  Can you get me my freshman yearbook?',\n",
              " 'L1041': 'Because I like to torture you.',\n",
              " 'L1040': 'Why is my veggie burger the only burnt object on this grill?',\n",
              " 'L1035': 'Yeah, but then I fucked up. I fell for her.',\n",
              " 'L1034': 'Is that right?',\n",
              " 'L1033': 'Besides, I had some extra cash. Some asshole paid me to take out a really great girl.',\n",
              " 'L1032': 'I thought you could use it. When you start your band.',\n",
              " 'L1031': 'A Fender Strat. You bought this?',\n",
              " 'L977': \"I didn't care about the money.\",\n",
              " 'L976': 'Really?  What was it like?  A down payment now, then a bonus for sleeping with me?',\n",
              " 'L975': \"It wasn't like that.\",\n",
              " 'L974': 'You were paid to take me out!  By -- the one person I truly hate.  I knew it was a set-up!',\n",
              " 'L973': 'Wait I...',\n",
              " 'L961': 'It gets worse -- you still have your freshman yearbook?',\n",
              " 'L960': \"That ' s completely adorable!\",\n",
              " 'L959': \"That's where I was last year.  She'd never lived alone -- my grandfather died -- I stayed with her.  I wasn't in jail, I don't know Marilyn Manson, and I've never slept with a Spice Girl.  I spent a year sitting next to my grandma on the couch watching Wheel of Fortune.  End of story.\",\n",
              " 'L958': 'What?',\n",
              " 'L957': \"My grandmother's .\",\n",
              " 'L940': \"Look, I'm  -- sorry -- that I questioned your motives.  I was wrong.\",\n",
              " 'L939': 'Oh huh',\n",
              " 'L938': \"It's just something I had.  You know\",\n",
              " 'L937': \"It's Scurvy's.  His date got convicted. Where'd you get the dress?\",\n",
              " 'L936': \"How'd you get a tux at the last minute?\",\n",
              " 'L857': \"Nothing!  There's nothing in it for me. Just the pleasure of your company.\",\n",
              " 'L856': 'Answer the question, Patrick',\n",
              " 'L855': 'You need therapy.  Has anyone ever told you that?',\n",
              " 'L854': 'You tell me.',\n",
              " 'L853': 'So I have to have a motive to be with you?',\n",
              " 'L852': 'Create a little drama?  Start a new rumor?  What?',\n",
              " 'L848': \"Because I don't want to. It's a stupid tradition.\",\n",
              " 'L847': 'Why not?',\n",
              " 'L846': \"No, I won't go with you\",\n",
              " 'L845': 'No what?',\n",
              " 'L844': 'No.',\n",
              " 'L843': 'You know what I mean',\n",
              " 'L842': 'Is that a request or a command?',\n",
              " 'L841': 'Go to the prom with me',\n",
              " 'L840': \"You're amazingly self-assured. Has anyone ever told you that?\",\n",
              " 'L839': 'No one else knows',\n",
              " 'L838': 'What?',\n",
              " 'L837': \"You're sweet.  And sexy.  And completely hot for me.\",\n",
              " 'L836': 'No -- something real.  Something no one else knows.',\n",
              " 'L835': 'I hate peas.',\n",
              " 'L834': 'Tell me something true.',\n",
              " 'L832': \"I know the porn career's a lie.\",\n",
              " 'L831': 'Hearsay.',\n",
              " 'L830': 'The duck?',\n",
              " 'L829': 'Fallacy.',\n",
              " 'L828': 'State trooper?',\n",
              " 'L825': 'For. . . ?',\n",
              " 'L824': 'You up for it?',\n",
              " 'L823': 'You never disappointed me.',\n",
              " 'L822': 'How?',\n",
              " 'L821': 'Then you screwed up',\n",
              " 'L820': 'Something like that',\n",
              " 'L819': \"So if you disappoint them from the start, you're covered?\",\n",
              " 'L818': \"I don't like to do what people expect. Then they expect it all the time and they get disappointed when you change.\",\n",
              " 'L817': 'Yes',\n",
              " 'L816': 'Acting the way we do.',\n",
              " 'L815': \"So what's your excuse?\",\n",
              " 'L814': \"Yeah, well, don't let it get out\",\n",
              " 'L813': 'A soft side? Who knew?',\n",
              " 'L812': 'I dazzled him with my wit',\n",
              " 'L811': \"So how'd you get Chapin to look the other way?\",\n",
              " 'L810': 'Good call.',\n",
              " 'L809': 'I figured it had to be something ridiculous to win your respect.  And piss you off.',\n",
              " 'L808': 'The Partridge Family?',\n",
              " 'L806': 'Maybe.',\n",
              " 'L805': 'You want me to climb up and show you how to get down?',\n",
              " 'L804': \"Forget it.  I'm stayin'.\",\n",
              " 'L803': 'Put your right foot there --',\n",
              " 'L802': \"Try lookin' at it from this angle\",\n",
              " 'L801': \"C'mon.  It's not that bad\",\n",
              " 'L800': \"I guess I never told you I'm afraid of heights.\",\n",
              " 'L799': 'Look up, sunshine',\n",
              " 'L798': 'He left!  I sprung the dickhead and he cruised on me.',\n",
              " 'L774': 'Other than my upchuck reflex? Nothing.',\n",
              " 'L773': 'So what did I have an effect on ?',\n",
              " 'L772': \"Don't for one minute think that you had any effect whatsoever on my panties.\",\n",
              " 'L771': 'Unwelcome?  I guess someone still has her panties in a twist.',\n",
              " 'L770': 'Unwelcome.',\n",
              " 'L769': 'Wholesome.',\n",
              " 'L768': 'Pleasant?',\n",
              " 'L767': \"You 're so --\",\n",
              " 'L766': 'I heard there was a poetry reading.',\n",
              " 'L765': 'What are you doing here?',\n",
              " 'L764': 'Excuse me, have you seen The Feminine Mystique?  I lost my copy.',\n",
              " 'L690': \"No offense, but you're sister is without.  I know everyone likes her and all, but ...\",\n",
              " 'L689': 'BIANCA',\n",
              " 'L688': 'Who?',\n",
              " 'L687': \"He just wants me to be someone I'm not.\",\n",
              " 'L686': \"So what ' s up with your dad?  He a pain in the ass?\",\n",
              " 'L684': \"I'm gettin' there\",\n",
              " 'L683': 'Oh, so now you think you know me?',\n",
              " 'L682': \"You don't strike me as the type that would ask permission.\",\n",
              " 'L681': \"My father wouldn't approve of that that\",\n",
              " 'L680': 'Start a band?',\n",
              " 'L679': 'This.',\n",
              " 'L678': 'Do what?',\n",
              " 'L677': 'I should do this.',\n",
              " 'L676': \"Why'd you lie?\",\n",
              " 'L675': \"Then why'd you ask?\",\n",
              " 'L674': \"No, you weren't\",\n",
              " 'L673': 'Maybe.',\n",
              " 'L672': 'Were you in jail?',\n",
              " 'L671': 'Busy',\n",
              " 'L670': 'When you were gone last year -- where were you?',\n",
              " 'L669': \"But it's Gigglepuss - I know you like them.  I saw you there.\",\n",
              " 'L668': \"And I'm in control of it.\",\n",
              " 'L651': 'What?',\n",
              " 'L650': 'Kat! Wake up!',\n",
              " 'L646': 'You know what they say',\n",
              " 'L645': 'I thought you were above all that',\n",
              " 'L644': 'Hey man. . .  You don \\' t think I can be \"cool\"?  You don\\'t think I can be \"laid back\" like everyone else?',\n",
              " 'L643': \"I know.  It'd have to be a pretty big deal to get you to mainline tequila. You don't seem like the type.\",\n",
              " 'L642': 'I hate him.',\n",
              " 'L641': 'Dorsey.',\n",
              " 'L640': 'Who?',\n",
              " 'L639': \"Why'd you let him get to you?\",\n",
              " 'L636': 'Just let me sit down.',\n",
              " 'L635': \"See that?  Who needs affection when I've got blind hatred?\",\n",
              " 'L634': 'Like you could find one',\n",
              " 'L633': \"Because then I'd have to start taking out girls who like me.\",\n",
              " 'L632': 'Why?',\n",
              " 'L631': 'Sure, I do',\n",
              " 'L630': \"You don't care if I die\",\n",
              " 'L629': 'I told you',\n",
              " 'L628': \"Why 're you doing this?\",\n",
              " 'L627': \"Leave it to you to use big words when you're shitfaced.\",\n",
              " 'L626': 'This is so patronizing.',\n",
              " 'L616': 'What if you have a concussion? My dog went to sleep with a concussion and woke up a vegetable. Not that I could tell the difference...',\n",
              " 'L615': 'I know, just let me sleep',\n",
              " 'L614': \"Uh, uh. You lie down and you'll go to sleep\",\n",
              " 'L613': 'I just need to lie down for awhile',\n",
              " 'L612': \"You're not okay.\",\n",
              " 'L611': \"I'm fine. I'm\",\n",
              " 'L610': 'Okay?',\n",
              " 'L567': \"Funny, you're the only one\",\n",
              " 'L566': 'I say, do what you wanna do.',\n",
              " 'L565': '\"I\\'m getting trashed, man.\" Isn\\'t that what you\\'re supposed to do at a party?',\n",
              " 'L564': \"What's this?\",\n",
              " 'L483': \"Why, don't you?\",\n",
              " 'L482': 'You know who The Raincoats are?',\n",
              " 'L481': \"You know, these guys are no Bikini Kill or The Raincoats, but they're right up there.\",\n",
              " 'L477': \"Do you mind?  You're sort of ruining it for me.\",\n",
              " 'L476': \"That's what you want, isn't it?\",\n",
              " 'L475': 'Excuse me?',\n",
              " 'L474': '',\n",
              " 'L473': 'hey.  Great show, huh?',\n",
              " 'L334': \"Depends on the topic. My fenders don't really whip me into a verbal frenzy.\",\n",
              " 'L333': \"You're not a big talker, are you?\",\n",
              " 'L332': 'Hi',\n",
              " 'L331': \"I was in the laundromat. I saw your car. Thought I'd say hi.\",\n",
              " 'L330': 'Are you following me?',\n",
              " 'L328': 'Seven-thirty?',\n",
              " 'L327': 'You -- covered in my vomit.',\n",
              " 'L326': 'Come on -- the ponies, the flat beer, you with money in your eyes, me with my hand on your ass...',\n",
              " 'L325': 'And why would I do that?',\n",
              " 'L324': \"Then say you'll spend Dollar Night at the track with me.\",\n",
              " 'L323': \"I don't really think you warrant that strong an emotion.\",\n",
              " 'L322': \"You hate me don't you?\",\n",
              " 'L313': 'I know a lot more than that',\n",
              " 'L312': 'Like where?  The 7-Eleven on Burnside? Do you even know my name, screwboy?',\n",
              " 'L311': \"The night I take you to places you've never been before.  And back.\",\n",
              " 'L310': 'Oh, right.  Friday.',\n",
              " 'L309': 'Pick you up Friday, then',\n",
              " 'L307': 'My mission in life.',\n",
              " 'L306': \"There's a way to get a guy's attention.\",\n",
              " 'L305': 'Sweating like a pig, actually.  And yourself?',\n",
              " 'L304': \"I mean Wo-man.  How ya doin'?\",\n",
              " 'L1000': \"Oh, Christ.  Don't tell me you've changed your mind.  I already sent 'em a check.\",\n",
              " 'L999': 'When I go?',\n",
              " 'L998': \"You know, fathers don't like to admit that their daughters are capable of running their own lives.  It means we've become spectators.  Bianca still lets me play a few innings.  You've had me on the bleachers for years.  When you go to Sarah Lawrence, I won't even be able to watch the game.\",\n",
              " 'L997': 'No -- impressed.',\n",
              " 'L996': \"What's the matter?  Upset that I rubbed off on her?\",\n",
              " 'L995': 'Bianca did what?',\n",
              " 'L994': 'The part where Bianca beat the hell out of some guy.',\n",
              " 'L993': 'Which parts?',\n",
              " 'L992': 'Parts of it.',\n",
              " 'L991': 'So tell me about this dance. Was it fun?',\n",
              " 'L990': 'No, Daddy.',\n",
              " 'L989': \"I don't understand the allure of dehydrated food.  Is this something I should be hip to?\",\n",
              " 'L988': 'Funny.',\n",
              " 'L987': 'Yeah.  She left with some bikers Big ones.  Full of sperm.',\n",
              " 'L986': 'Was that your sister?',\n",
              " 'L347': 'I want to go to an East Coast school! I want you to trust me to make my own choices.  I want --',\n",
              " 'L346': \"You're eighteen.  You don't know what you want.  You won't know until you're forty-five and you don't have it.\",\n",
              " 'L345': \"So what I want doesn't matter?\",\n",
              " 'L344': \"As a parent, that's my right\",\n",
              " 'L343': \"Because you're making decisions for me.\",\n",
              " 'L342': \"Why can't we agree on this?\",\n",
              " 'L341': 'I thought you were punishing me.',\n",
              " 'L340': 'Is this about Sarah Lawrence? You punishing me?',\n",
              " 'L339': 'Then tell them I had a seizure.',\n",
              " 'L338': 'My insurance does not cover PMS',\n",
              " 'L185': 'Enough!',\n",
              " 'L184': 'This from someone whose diary is devoted to favorite grooming tips?',\n",
              " 'L174': 'You decided.',\n",
              " 'L173': 'I thought we decided you were going to school here.  At U of 0.',\n",
              " 'L172': 'I know.',\n",
              " 'L1018': 'Just smack her now.',\n",
              " 'L1017': 'Am I supposed to feel better? Like, right now?  Or do I have some time to think about it?',\n",
              " 'L109': 'No ... I believe \"heinous bitch\" is the term used most often.',\n",
              " 'L108': 'Tempestuous?',\n",
              " 'L107': 'The point is Kat -- people perceive you as somewhat ...',\n",
              " 'L106': 'I still maintain that he kicked himself in the balls.  I was merely a spectator.',\n",
              " 'L105': \"Well, yes, compared to your other choices of expression this year, today's events are quite mild.  By the way, Bobby Rictor's gonad retrieval operation went quite well, in case you're interested.\",\n",
              " 'L104': 'Expressing my opinion is not a terrorist action.',\n",
              " 'L103': \"Katarina Stratford.  My, my.  You've been terrorizing Ms. Blaise again.\",\n",
              " 'L738': 'Yeah...',\n",
              " 'L737': 'Kat a fan, too?',\n",
              " 'L736': 'Right.',\n",
              " 'L735': 'Macbeth, right?',\n",
              " 'L733': 'Oh yeah.',\n",
              " 'L732': 'You think?',\n",
              " 'L731': 'Yeah.  I guess.',\n",
              " 'L730': 'Cool pictures.  You a fan?',\n",
              " 'L729': 'Hi.',\n",
              " 'L728': 'Hey there.  Tired of breathing?',\n",
              " 'L777': \"Man -- don't say shit like that to  me. People can hear you.\",\n",
              " 'L776': 'Sweet love, renew thy force!',\n",
              " 'L775': \"You were right. She's still pissed.\",\n",
              " 'L724': \"No - I've got a sweet-payin' job that I'm about to lose.\",\n",
              " 'L723': 'So you got cozy with she who stings?',\n",
              " 'L437': \"I'm likin' you guys better\",\n",
              " 'L436': 'I prefer to think of it simply as an alternative to what the law allows.',\n",
              " 'L429': 'Yeah.',\n",
              " 'L428': 'Ever been to Club Skunk?',\n",
              " 'L421': 'Just for now.',\n",
              " 'L420': 'Are you telling me I\\'m a -  \"non-smoker\"?',\n",
              " 'L415': 'Good enough.',\n",
              " 'L414': 'What?!',\n",
              " 'L384': \"We're your guys.\",\n",
              " 'L383': 'You two are gonna help me tame the wild beast?',\n",
              " 'L382': \"Patrick, Pat, you're not looking at the big picture.  Joey's just a pawn. We set this whole thing up so Cameron can get the girl.\",\n",
              " 'L381': 'So Dorsey can get the girl?',\n",
              " 'L380': \"That's where we can help you.  With Kat.\",\n",
              " 'L379': \"Dorsey can plow whoever he wants. I'm just in this for the cash.\",\n",
              " 'L378': \"I think I speak correctly when I say that Cameron's love is pure.  Purer than say -- Joey Dorsey's.\",\n",
              " 'L377': 'What is it with this chick?  She have three tits?',\n",
              " 'L376': 'The situation is, my man Cameron here has a major jones for Bianca Stratford.',\n",
              " 'L375': 'What plan?',\n",
              " 'L373': \"Whatever the hell it is you're standin' there waitin' to say.\",\n",
              " 'L372': 'What?',\n",
              " 'L371': 'Say it',\n",
              " 'L512': 'See you next week!',\n",
              " 'L511': \"You're completely demented.\",\n",
              " 'L264': \"Well, you know -- there's the prestige of the job title... and the benefits package is pretty good...\",\n",
              " 'L263': \"You weren't abused, you aren't stupid, and as far as I can tell, you're only slightly psychotic -- so why is it that you're such a fuck-up?\",\n",
              " 'L262': \"What's to discuss?\",\n",
              " 'L261': \"Why don't we discuss your driving need to be a hemorrhoid?\",\n",
              " 'L259': \"I'm at a loss, then.  What should we talk about? Your year of absence?\",\n",
              " 'L258': 'Touch of the flu.',\n",
              " 'L257': \"I don't understand, Patrick.  You haven't done anything asinine this week. Are you not feeling well?\",\n",
              " 'L62': 'With the teeth of your zipper?',\n",
              " 'L61': 'It was a bratwurst.  I was eating lunch.',\n",
              " 'L60': 'It says here you exposed yourself to a group of freshmen girls.',\n",
              " 'L59': 'I missed you.',\n",
              " 'L933': \"That ' s what I thought\",\n",
              " 'L932': 'Absolutely not.',\n",
              " 'L931': 'Did I have anything to say about it?',\n",
              " 'L930': 'Your daughters went to the prom.',\n",
              " 'L929': 'What just happened?',\n",
              " 'L927': 'But -- who -- what --?',\n",
              " 'L926': 'Have a great time, honey!',\n",
              " 'L918': 'Dr. Ruth?',\n",
              " 'L917': \"What do you wanna watch?  We've got crap, crap, crap or crap\",\n",
              " 'L887': \"Kissing?  Is that what you think happens?  Kissing isn't what keeps me up to my elbows in placenta all day.\",\n",
              " 'L886': \"They'll dance, they'll kiss, they'll come home.  Let her go.\",\n",
              " 'L877': 'Pirate -- no question.',\n",
              " 'L876': 'Would you rather be ravished by a pirate or a British rear admiral?',\n",
              " 'L193': \"You're not helping.\",\n",
              " 'L192': 'Tumescent!',\n",
              " 'L191': 'Jesus!  Can a man even grab a sandwich before you women start dilating?',\n",
              " 'L171': 'Sarah Lawrence is on the other side of the country.',\n",
              " 'L170': \"What's a synonym for throbbing?\",\n",
              " 'L162': 'Make anyone cry today?',\n",
              " 'L161': 'In the microwave.',\n",
              " 'L2181': \"Can't be that far, I say.  Also, I don't like the smell of the sea around here.  Smells like a cunt. Bad sign...\",\n",
              " 'L2180': \"We left three weeks ago, Alonso. Can't be that near.\",\n",
              " 'L2179': 'We should have seen land.',\n",
              " 'L2177': \"We'll all go crazy...\",\n",
              " 'L2176': \"He's the devil's child...\",\n",
              " 'L2175': \"With a face like that?  I don't want you looking at me.  You hear?\",\n",
              " 'L2174': \"Ah, leave him alone.  He's doing no harm.\",\n",
              " 'L2173': 'What are you listening to, chicken ass?',\n",
              " 'L2172': \"You'll be drinking your own piss... For the glory of Spain... and Admiral Colon...!  Bastard!\",\n",
              " 'L2171': \"The water's going putrid in the barrels.\",\n",
              " 'L2170': 'I never seen heat like this!  Not even in Las Minas!',\n",
              " 'L2025': 'IF-GOD-WILLS-IT!',\n",
              " 'L2024': 'Asia can be found to the west -- and I will prove it.',\n",
              " 'L2023': 'Blind faith is what I consider heresy!',\n",
              " 'L2022': \"Don't you realize your words could be considered heretical?\",\n",
              " 'L2020': \"Did He not choose a carpenter's son to reveal Himself to the world?\",\n",
              " 'L2019': 'If God intended our proximity to Asia, do you believe he would have waited for you to show it to the world?',\n",
              " 'L2016': 'No.  The Portuguese have already discovered black-skinned people.  I, too, will find other populations -- and bring them to the word of God.',\n",
              " 'L2015': 'Is that all that interests you? Gold?',\n",
              " 'L2014': 'Trade, Your Excellency.  According to Marco Polo, the Kingdom of China is one of the richest of the world. Even the meanest buildings are roofed with gold.',\n",
              " 'L2012': 'If they agree to follow me, yes.',\n",
              " 'L2011': 'Your life, and that of others!',\n",
              " 'L2010': 'Your Eminence, there is only one way to settle the matter.  And that is to make the journey.  I am ready to risk my life to prove it possible.',\n",
              " 'L2006': 'Excellency, you are right.',\n",
              " 'L2005': 'Senor Colon, an experienced captain such as yourself will understand our concern with the crew.  I am not willing to have on my conscience the loss of men who would have relied upon our judgment.',\n",
              " 'L1993': 'Then you cannot ignore that according to their calculations, the circumference of the Earth is approximately...  22,000 leagues or more.  Which makes the ocean... uncrossable.',\n",
              " 'L1992': 'I am, Your Eminence',\n",
              " 'L1991': 'Unfortunately, Don Colon, that is precisely where our opinions differ...  Are you familiar with the work of Aristotle?  Erathostene?  Ptolemeus?',\n",
              " 'L1990': 'Yes, your Eminence.  The voyage should not take more than six or seven weeks.',\n",
              " 'L1989': 'You say Asia can be found by sailing west?',\n",
              " 'L2524': 'A waste...?  Let me tell you something, Arojaz.  If your name, or mine, is ever remembered -- it will only be because of his.',\n",
              " 'L2523': 'What a tragedy... what a waste of a life...',\n",
              " 'L2522': 'You can see for yourself.',\n",
              " 'L2275': 'On the contrary, Your Eminence.  It seems to me the man is preparing his own cross.',\n",
              " 'L2274': \"It won't be easy to get rid of your prophet now, Don Sanchez.\",\n",
              " 'L2031': 'Indeed.  The world is full of mercenaries -- and states often make use of them, when it benefits them.  My only concern is the welfare and prosperity of Spain.',\n",
              " 'L2030': 'He is a mercenary!  Did he not already try to convince the King of Portugal of his absurd notions?',\n",
              " 'L2029': 'Naturally.  But I would really deplore the loss of such a potential opportunity for Spain for a... dispute over a point of geography.',\n",
              " 'L2028': 'The Judgment is ours!',\n",
              " 'L2027': \"The State has some reason to be interested in this man's proposition, Your Eminence...\",\n",
              " 'L2540': \"I can't keep my eyes off you.  I would like to catch up with all the moments I didn't spend with you.\",\n",
              " 'L2539': 'What is it, now?  Tell me...',\n",
              " 'L2538': 'I am busy inside.',\n",
              " 'L2537': \"Can't you stay with us a little?\",\n",
              " 'L2474': \"Not everything... Do you think I care?  I'm a free man again.  Riches don't make a man rich, they only make him busier...\",\n",
              " 'L2473': 'They took everything...',\n",
              " 'L2472': \"They tried... but I didn't let them.\",\n",
              " 'L2471': \"God... you're so beautiful!  I can't believe no other man has ever taken you away from me...\",\n",
              " 'L2320': 'I can arrange for the Queen to take Fernando and Diego into her service.',\n",
              " 'L2319': \"You don't usually ask.\",\n",
              " 'L2318': 'Beatrix, I want to ask you something.',\n",
              " 'L2123': \"That's something you can't decide.\",\n",
              " 'L2122': \"I don't want you to wait for me.\",\n",
              " 'L2121': \"I'm not asking you to swear to anything.\",\n",
              " 'L2119': 'Thank God...',\n",
              " 'L2118': 'She said yes.',\n",
              " 'L1986': 'I find that hard to believe.',\n",
              " 'L1985': 'Perhaps I was never meant to live with a woman...',\n",
              " 'L1984': \"I'd love to argue with you sometimes.  But you're never here!\",\n",
              " 'L1983': 'Are we going to argue?',\n",
              " 'L1982': \"Well... that's true.  I have a child by a man who won't marry me!  Who's always leaving...\",\n",
              " 'L1981': \"I haven't given you much of a life.\",\n",
              " 'L1980': 'I know.',\n",
              " 'L1979': 'I could be gone for years.',\n",
              " 'L2432': 'I am afraid this is not the worst news.',\n",
              " 'L2431': 'How could I be?  The mainland has been found.  Exactly as I said it would.',\n",
              " 'L2430': 'I am not a seaman.  But I heard it is no more than a week at sea.  I hope you are not too disappointed.',\n",
              " 'L2429': 'How far from here?',\n",
              " 'L2426': 'Congratulations.  Then I am free to search for the mainland.',\n",
              " 'L2425': 'Viceroy of the West Indies.',\n",
              " 'L2424': 'Appointment to what?',\n",
              " 'L2423': 'My letters of appointment.',\n",
              " 'L2422': 'Yes... I remember...',\n",
              " 'L2421': 'Don Alonso de Bobadilla.',\n",
              " 'L2293': 'Bartolome and Giacomo Colon.',\n",
              " 'L2292': 'May I ask by whom?',\n",
              " 'L2291': 'Forgive me, Don Bobadilla -- those positions have already been taken.',\n",
              " 'L2290': 'I understand that you will soon be appointing Governors for the islands?  Is it not so?',\n",
              " 'L2552': 'Tell me the first thing that comes to your mind.',\n",
              " 'L2551': \"Really?  God... I wouldn't know where to start... and yet...\",\n",
              " 'L2550': 'I want you to tell me everything you remember, Father.  From the beginning.  Everything.',\n",
              " 'L2549': 'He never had one... except aboard my ships!',\n",
              " 'L2548': 'He asks when he can come to visit you.  He left his address.',\n",
              " 'L2547': 'What does he say?',\n",
              " 'L2542': \"I am not listening, Father.  But I can't help hearing.\",\n",
              " 'L2541': 'What are you listening to?',\n",
              " 'L2501': 'There must be a passage to that other ocean.',\n",
              " 'L2500': 'Father...',\n",
              " 'L2485': 'Not bad.',\n",
              " 'L2484': 'How are you feeling, Fernando?',\n",
              " 'L2440': 'This time with me!',\n",
              " 'L2439': 'I have to explore the mainland.',\n",
              " 'L2131': 'Yes... Yes, I do... On all of them!',\n",
              " 'L2130': 'Do you swear on all the Holy Saints in heaven?',\n",
              " 'L2129': 'You promise?  Do you swear on St. Christopher...?',\n",
              " 'L2128': \"There'll be a time.\",\n",
              " 'L2127': 'I want to go with you!',\n",
              " 'L2469': 'All of them created by people like me.',\n",
              " 'L2468': 'Roofs... towers, palaces... spires...',\n",
              " 'L2467': 'What do you see?',\n",
              " 'L2466': 'Look out of that window.',\n",
              " 'L2465': 'I am not afraid of you.  You are nothing but a dreamer.',\n",
              " 'L2464': 'Call them.',\n",
              " 'L2463': 'All I have to do is call the guards.',\n",
              " 'L2301': 'To rise so high, in so short a time, is a dangerous occupation.  A little hypocrisy goes a long way.',\n",
              " 'L2300': 'What...?  Do I have so many already?',\n",
              " 'L2299': 'You seem to have a special talent for making friends.',\n",
              " 'L2297': 'Good!  We are also in need of judges.  Except there are no thieves!',\n",
              " 'L2296': 'Don Bobadilla is already a judge, my Dear Don Cristobal.',\n",
              " 'L2295': 'But we do have a lack of notaries. You should contact my administration.',\n",
              " 'L2287': '... for a commoner?',\n",
              " 'L2286': 'You defend yourself admirably...',\n",
              " 'L2282': 'Forgive me, Don Colon.  But what about gold?',\n",
              " 'L2281': \"They don't see sin in their nakedness.  They live according to nature, in a never ending summer. The islands are covered with trees, filled with blossoms and fruits. And...\",\n",
              " 'L2110': \"If you won't accept our proposal, we'll simply find someone who will.\",\n",
              " 'L2109': 'And were you never ambitious, Excellency?  Or is ambition only a virtue among the nobles, a fault for the rest of us?',\n",
              " 'L2108': 'Then you are too ambitious.',\n",
              " 'L2107': \"I'm not bargaining!\",\n",
              " 'L2106': 'I remind you, Senor Colon, that you are in no position to bargain with me.',\n",
              " 'L2105': 'NO...!  I have waited too long, fought too hard.  Now you expect me to take all the risks while you take the profit!  No... I will not be your servant!',\n",
              " 'L2104': 'No?',\n",
              " 'L2103': 'No...',\n",
              " 'L2410': 'You never learned how to speak my language.',\n",
              " 'L2409': \"Utapan, won't you speak to me?  You used to know how to speak to me.\",\n",
              " 'L2394': 'You did the same to your God!',\n",
              " 'L2393': 'You have to find them, Utapan.  Look what they did!',\n",
              " 'L2343': 'Ask him if he will help.',\n",
              " 'L2342': 'He understands.',\n",
              " 'L2341': 'We will work with his people.  We want peace.  Ask the Chief if he understands?',\n",
              " 'L2260': 'He has medicine.  Tell him we admire his people.',\n",
              " 'L2259': 'Chief says...',\n",
              " 'L2258': '... and also to bring medicine.',\n",
              " 'L2257': 'Chief says -- he has a God.',\n",
              " 'L2256': 'To bring the word of God.',\n",
              " 'L2255': 'Why?',\n",
              " 'L2254': 'Thousands.',\n",
              " 'L2253': 'Chief says -- how many?',\n",
              " 'L2251': 'Tell him his country is very beautiful.  Tell him we are leaving men here -- to build a fort.',\n",
              " 'L2250': 'Chief knows.',\n",
              " 'L2249': 'Tell the Chief we thank him.',\n",
              " 'L2248': 'You come!  You speak first!',\n",
              " 'L2239': 'Island.  Far.',\n",
              " 'L2238': 'What is it?  A tribe?  An island?',\n",
              " 'L2237': 'Say not here!  Cuba!',\n",
              " 'L2535': \"I don't know... I have the impression that I didn't change that much.  I still can't accept the world as it is!\",\n",
              " 'L2534': 'Oh?  So you are a new man?',\n",
              " 'L2533': 'New worlds create new people.',\n",
              " 'L2532': 'I knew you would.',\n",
              " 'L2531': 'I have to disagree.',\n",
              " 'L2526': \"You'll always be older than me, Father.\",\n",
              " 'L2525': \"I suppose we're both old men now.\",\n",
              " 'L2145': 'Give me absolution.',\n",
              " 'L2144': 'I believed in you...',\n",
              " 'L2143': 'You are bound by an oath, Father.',\n",
              " 'L2142': \"My son, my son...  Your certitudes are sometimes frightening...  Christopher, you must speak to them. And if you don't I will.\",\n",
              " 'L2141': \"If I tell them, they won't follow me.  You know that I am right, Father.  You trust me...\",\n",
              " 'L2140': 'May God forgive you...!  You must tell them!  You must tell your men!',\n",
              " 'L2139': 'I am not sure... It could be twice the distance.',\n",
              " 'L2138': 'How long?',\n",
              " 'L2137': 'I lied.  The journey will be longer than I said.',\n",
              " 'L2136': 'What are you saying?',\n",
              " 'L2135': 'Father, I have betrayed my family. I betrayed my men.  And I betrayed you.',\n",
              " 'L2134': 'I am listening, my son.',\n",
              " 'L2133': 'Forgive me, Father.  For I have sinned.',\n",
              " 'L2132': 'In Nomine Patris et Filius, et Spiritus Sancti.',\n",
              " 'L2046': \"Colon!  Don't!\",\n",
              " 'L2045': 'All of them!  Just lies!',\n",
              " 'L2042': 'Damn all of you!  You all set up theories based on what?  You never leave the safety of your studies! Go out!  Find out what the world is about and then tell me something I can listen to!',\n",
              " 'L2041': 'Colon!',\n",
              " 'L2040': 'Damn God!',\n",
              " 'L2039': 'If God intends you to go, then you will go.',\n",
              " 'L2038': \"Wait!  I've waited seven years already!  How much longer do you want me to wait?\",\n",
              " 'L2037': \"You mustn't give way to despair. You must wait.\",\n",
              " 'L1969': 'Only God knows the meaning of such words, my son.',\n",
              " 'L1968': \"I've been contradicted all my life... Eternity!\",\n",
              " 'L1967': 'You get so carried away when you are being contradicted!',\n",
              " 'L1966': 'Passion is something one cannot control!',\n",
              " 'L1965': 'Father Marchena!',\n",
              " 'L1964': \"I'll try to remember that, Marchena...\",\n",
              " 'L1963': \"Two minutes... and already you're a dead man.  Don't let passion overwhelm you, Colon.\",\n",
              " 'L1962': 'So was Christ!',\n",
              " 'L1961': 'Esdras is a Jew.',\n",
              " 'L1960': 'The calculations of Toscanelli Marin de Tyr, Esdras...',\n",
              " 'L1959': 'How can you be so certain?',\n",
              " 'L1958': 'Ignorance!  I believe the Indies are no more than 750 leagues west of the Canary Islands.',\n",
              " 'L1957': 'How can you be so certain?  The Ocean is said to be infinite.',\n",
              " 'L1952': 'To open a new route to Asia.  At the moment there are only two ways of reaching it...',\n",
              " 'L1951': 'Why do you wish to sail west?',\n",
              " 'L1950': 'With some difficulty.  I had to promise them you were not a total fool.',\n",
              " 'L1949': 'How did you manage it?',\n",
              " 'L1948': \"That's what it says.\",\n",
              " 'L1947': \"God... That's in a week!\",\n",
              " 'L1943': 'Father, I am doing what I think is the best for him.  And he has the teacher I would have chosen for myself.',\n",
              " 'L1942': 'Diego is a bright boy -- a pleasure to teach -- but so serious... Brothers should be raised together, Colon.  Even brothers from different mothers...',\n",
              " 'L2459': 'Your Majesty -- some men are content to read about things.  I must see them with my own eyes.  I cannot be other than I am.',\n",
              " 'L2458': \"There is one thing I'd like to understand... Why do you want to go back, after all this?\",\n",
              " 'L2457': 'Thank you.',\n",
              " 'L2456': 'But without your brothers.  Nor are you to return to Santo Domingo or any of the other colonies.  You may explore the continent.',\n",
              " 'L2280': 'They come and go as naked as the day God created them...',\n",
              " 'L2279': 'Do they have such thoughts?',\n",
              " 'L2086': 'Thirty seven, Your Majesty... And you?',\n",
              " 'L2085': 'How old are you, Senor Colon?',\n",
              " 'L2082': 'A woman?',\n",
              " 'L2081': 'I know what I see.  I see someone who doesn\\'t accept the world as it is.  Who\\'s not afraid.  I see a women who thinks... \"What if?\"...',\n",
              " 'L2080': 'You show no inclination to speak otherwise!',\n",
              " 'L2079': 'May I speak freely?',\n",
              " 'L2077': 'Surely you can do anything you want.',\n",
              " 'L2076': 'I cannot ignore the verdict of my council.',\n",
              " 'L2075': 'That she was impregnable.',\n",
              " 'L2074': 'What did they say about Granada before today?',\n",
              " 'L2073': 'The ocean is uncrossable?',\n",
              " 'L2072': 'No more than the woman who said she would take Granada from the Moors.',\n",
              " 'L2071': 'I should not even be listening to you, since my council said no.  But Santangel tells me you are a man of honor and sincerity... And Sanchez, that you are not a fool.',\n",
              " 'L2389': 'You will regret this.',\n",
              " 'L2388': \"You'll be held in detention, deprived of your privileges and possessions.  Until you are returned to Spain where you will be judged. Have you anything to say?\",\n",
              " 'L2387': 'Savagery is what monkeys understand.',\n",
              " 'L2386': 'In one act of brutality, you have created chaos.  Tribes who were fighting each other are now joining forces against us!  All that because of your criminal savagery!',\n",
              " 'L2348': 'You did not hear me, Don Colon.  Not my horse.',\n",
              " 'L2347': 'Don Moxica -- we all have to work.',\n",
              " 'L2346': \"My horse doesn't work.\",\n",
              " 'L2345': \"We can't raise the wheel without it.\",\n",
              " 'L2333': 'We came here to stay!  To build! Not to start a crusade.  In this forest, there is enough danger to sweep us away in days!  So we will be brave and swallow our grief.  And in the name of those who died, we will accomplish what we came for.',\n",
              " 'L2332': \"We don't need to know.\",\n",
              " 'L2331': 'You want a war?  Fine.  We are a thousand.  They outnumber us by ten! Who will you kill?  Which tribe?',\n",
              " 'L2329': \"If you want to keep your head on your shoulders, you'll do as I say.\",\n",
              " 'L2328': 'We lost cousins, friends.  We will wash this in blood.',\n",
              " 'L2278': 'The Indians have no such word, Don Moxica.',\n",
              " 'L2277': \"And you say this is an Indian vice? By God!  I don't see any kind of pleasure that would make this a sin.\",\n",
              " 'L2492': \"We can't be.\",\n",
              " 'L2491': \"He's drawing an isthmus... He's saying we're on an isthmus.\",\n",
              " 'L2490': \"What's he doing?\",\n",
              " 'L2165': 'Twenty eight.',\n",
              " 'L2164': 'What do you read?',\n",
              " 'L2160': 'Come over here.',\n",
              " 'L2159': \"Well, I surely know what a quadrant is!  But I've never seen it used at night before.\",\n",
              " 'L2158': 'And what do you think Mendez?',\n",
              " 'L2157': \"Well... It's the men, Sir.  They wonder how you know our position. We've lost sight from land days ago...\",\n",
              " 'L2155': 'God be with us admiral.',\n",
              " 'L2154': 'Due west, Captain Mendez.  And may God be with us...',\n",
              " 'L2211': \"You're right.  Let the men decide.\",\n",
              " 'L2210': 'You tell that to them!',\n",
              " 'L2209': 'Pinzon, Pinzon... All we can do now is go forward!  Think about that!',\n",
              " 'L2208': 'You bloody...',\n",
              " 'L2207': 'You never did.  You did all the talking for both of us, remember?',\n",
              " 'L2206': 'Jesus Maria!  I should have never listened to you!',\n",
              " 'L2205': 'And then what?  Half of the water has gone, the rest is nearly putrid! You know that!',\n",
              " 'L2204': \"You don't know anything!  Listen Colon, these are my ships, right? So I'm telling you we're turning back!\",\n",
              " 'L2203': 'The land is there.  I know it!',\n",
              " 'L2202': \"We're lost!\",\n",
              " 'L2201': \"You think I don't know that?\",\n",
              " 'L2200': \"We're on the verge of a mutiny, Colon!\",\n",
              " 'L2199': 'We have to keep the hopes of these men alive!',\n",
              " 'L2198': 'You must be mad...!',\n",
              " 'L2197': 'Six days ago, yes.',\n",
              " 'L2196': \"You lied!  You cheated!  We're way past 750 leagues!\",\n",
              " 'L2065': 'Immediately.',\n",
              " 'L2064': 'Where can I meet this man?',\n",
              " 'L2462': 'Because he is not afraid of me.',\n",
              " 'L2461': 'Then why?',\n",
              " 'L2460': 'I know, I should not tolerate his impertinence.',\n",
              " 'L2419': 'And who would you think of, for such a task?',\n",
              " 'L2418': 'He must be replaced.',\n",
              " 'L2417': 'Then, what do you suggest, Don Sanchez?',\n",
              " 'L2415': 'Is this true, Brother Buyl?',\n",
              " 'L2414': '... But there is worse.  He ordered the execution of five members of the nobility...',\n",
              " 'L2377': \"We weren't expecting immediate profits, were we?  We must have faith.  We must give time for time.\",\n",
              " 'L2376': 'Every ship returns with a cargo of sick and dying.  But with no gold! The new world proves expensive, Your Majesty.',\n",
              " 'L2116': \"Yes.  It would be a pity, wouldn't it?  Call him back!\",\n",
              " 'L2115': '... Into a monk...',\n",
              " 'L2113': 'Never, Your Majesty.  Although...',\n",
              " 'L2112': 'You were right, Don Sanchez... His demands could never be granted.',\n",
              " 'L1931': 'Yes, Your Majesty.',\n",
              " 'L1930': 'Is that the man I knew, Treasurer Sanchez?',\n",
              " 'L3546': \"Officers, there's your killer, do your duty, arrest him!\",\n",
              " 'L3545': '...so we kill someone famous and if we are caught, we are sent to mental hospital...',\n",
              " 'L3497': \"I don't think it's abuse, I think it's torture.\",\n",
              " 'L3496': \"I'm abused.  Don't you think?\",\n",
              " 'L3493': 'Can I see your back?',\n",
              " 'L3492': 'Out on my back when I was a small boy.',\n",
              " 'L3491': 'Your father put cigarettes out on you?',\n",
              " 'L3490': \"That's what he did to me.  He put cigarettes out on me.\",\n",
              " 'L3489': 'Yeah, he hated me from day when I was born.  Put it out.  Can you put the cigarette out?',\n",
              " 'L3488': \"Your father blamed you for your mother's blindness?\",\n",
              " 'L3487': 'Yeah, yeah...bad doctor gave her bad drugs which made her go blind.  And my father blamed me for her blindness...',\n",
              " 'L3486': 'Back in the Czech Republic?',\n",
              " 'L3485': 'Yeah, she went blind giving birth to me. She went to fucking black market doctor to induce me.',\n",
              " 'L3484': 'Your mother was blind?',\n",
              " 'L3483': 'My father always degraded me.  Killed my self-esteem.  And my mother was blind.',\n",
              " 'L3482': 'Tell me about yourself.  What you did as a young boy... what your parents were like.',\n",
              " 'L3481': 'Give me another one, please.',\n",
              " 'L3480': \"I need to know about your background.  I need to know about your upbringing.  Why you're here.\",\n",
              " 'L3479': 'What else do you need?',\n",
              " 'L3478': 'This is not about money, Emil.  I need your trust in me.',\n",
              " 'L3477': \"Thirty-percent.  No more.  Or I call another lawyer.  This is the biggest case of your life.  Don't try to negotiate.  Thirty percent.  Say yes or no.\",\n",
              " 'L3476': \"But it's...\",\n",
              " 'L3475': 'No.  No way.',\n",
              " 'L3474': 'I would say...half.  Half is fair.',\n",
              " 'L3473': \"What's your cut?  How much?\",\n",
              " 'L3472': \"Look, I haven't really focused on that kind of thing.\",\n",
              " 'L3471': 'What about my movie rights?  Book rights?',\n",
              " 'L3470': \"Don't worry about him.  Think about yourself.\",\n",
              " 'L3469': 'No, he is here.  Shit...',\n",
              " 'L3468': \"Disappeared.  They're looking everywhere.  Maybe he went back to Czechoslovakia.\",\n",
              " 'L3467': 'What about Oleg?',\n",
              " 'L3466': \"Well, you didn't appreciate the severity of it until recently.  No question about that.\",\n",
              " 'L3465': 'I was all of these.',\n",
              " 'L3464': '...delusions and paranoia.',\n",
              " 'L3461': 'Oh, sure.',\n",
              " 'L3460': 'You bring the cigarettes?',\n",
              " 'L3459': \"I brought you some letters.  It's really fan mail.  Women mostly.  One wants to buy you clothes, another sent a check. Another wants a check.\",\n",
              " 'L3396': \"I'm invoking rights - this man is represented by counsel.  I'm coming with him.\",\n",
              " 'L3395': 'Yes.  Yes, come with me!',\n",
              " 'L3394': \"I'm coming with you.\",\n",
              " 'L3393': 'Where are we going?',\n",
              " 'L3392': \"Don't say anything.\",\n",
              " 'L3385': 'He has the camera!  He took the movie!',\n",
              " 'L3384': 'Emil.  Take it easy.  Stay with me.  Sit down.  What do you need?  What are you looking for?',\n",
              " 'L3383': 'Oh no!  No!  Shit!',\n",
              " 'L3382': 'Here.  I have your money.',\n",
              " 'L3381': \"I'm not your lawyer until I see the money.\",\n",
              " 'L3380': \"Are you my attorney?  I'm Emil.  I'm insane.\",\n",
              " 'L3517': 'Daphne, I...',\n",
              " 'L3516': \"I don't want to drag you down with me.\",\n",
              " 'L3515': '...Do you really want me to forget about you?',\n",
              " 'L3514': 'Forget about me.  You have enough problems of your own.',\n",
              " 'L3330': 'Pouring it out!',\n",
              " 'L3329': 'What are you doing?',\n",
              " 'L3318': \"I'll get my clothes.\",\n",
              " 'L3317': \"I'll make some for us.\",\n",
              " 'L3316': 'In the kitchen.',\n",
              " 'L3315': 'Do you have coffee?',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84KUfedGRNgJ",
        "colab_type": "text"
      },
      "source": [
        "## creating a list of all the conversations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhbCZA66Qj8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApKnBw-DWgPu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "47c7a024-5191-443c-c3b9-fb27b1171520"
      },
      "source": [
        "print(conversations_ids)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6dfaVdhWzh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line[conversation[i]])\n",
        "        answers.append(id2line[conversation[i+1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV9quIZGZHgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ques_df=pd.DataFrame(questions)\n",
        "ans_df=pd.DataFrame(answers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ls6Zv5pZYHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "70721433-51b9-4602-ca85-eec756513e70"
      },
      "source": [
        "ques_df.head(10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Cameron.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The thing is, Cameron -- I'm at the mercy of a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Why?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Unsolved mystery.  She used to be really popul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gosh, if only we could find Kat a boyfriend...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  Can we make this quick?  Roxanne Korrine and A...\n",
              "1  Well, I thought we'd start with pronunciation,...\n",
              "2  Not the hacking and gagging and spitting part....\n",
              "3  You're asking me out.  That's so cute. What's ...\n",
              "4  No, no, it's my fault -- we didn't have a prop...\n",
              "5                                           Cameron.\n",
              "6  The thing is, Cameron -- I'm at the mercy of a...\n",
              "7                                               Why?\n",
              "8  Unsolved mystery.  She used to be really popul...\n",
              "9     Gosh, if only we could find Kat a boyfriend..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb-Lr4d_Z8M8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d221a945-984f-4268-9716-7827a2cd84cb"
      },
      "source": [
        "ans_df.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Okay... then how 'bout we try out some French ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Forget it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cameron.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  Well, I thought we'd start with pronunciation,...\n",
              "1  Not the hacking and gagging and spitting part....\n",
              "2  Okay... then how 'bout we try out some French ...\n",
              "3                                         Forget it.\n",
              "4                                           Cameron."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmAw_lByaEfl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "403ab674-bb1d-4a16-b855-b8f64bb30a22"
      },
      "source": [
        "len(ques_df) , len(ans_df)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(221616, 221616)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5LwJ1rnaQs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bOS0aWEbnnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JroWIDhrbwQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYHBzp8Nbyqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filtering out the questions and answers that are too short or too long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jjqnZBuelOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-00sgRWeqtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 15\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbLapjh1gxlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SoFu9s2hWu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAVMZZ92hZzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B2-ZaL7hb6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "answers_into_int = []\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvwve9uwhhb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMAWVzl1hrsE",
        "colab_type": "text"
      },
      "source": [
        "# BUILDING THE SEQ2SEQ MODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clxluq07hn5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating placeholders for the inputs and the targets\n",
        "def model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
        "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "    return inputs, targets, lr, keep_prob\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGzgaxuPhvZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing the targets\n",
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vs9i4NThxd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Encoder RNN\n",
        "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
        "                                                                    cell_bw = encoder_cell,\n",
        "                                                                    sequence_length = sequence_length,\n",
        "                                                                    inputs = rnn_inputs,\n",
        "                                                                    dtype = tf.float32)\n",
        "    return encoder_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-IkxIz1h1y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoding the training set\n",
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              name = \"attn_dec_train\")\n",
        "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                              training_decoder_function,\n",
        "                                                                                                              decoder_embedded_input,\n",
        "                                                                                                              sequence_length,\n",
        "                                                                                                              scope = decoding_scope)\n",
        "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "    return output_function(decoder_output_dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6xdOoCQh5qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoding the test/validation set\n",
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                              encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              decoder_embeddings_matrix,\n",
        "                                                                              sos_id,\n",
        "                                                                              eos_id,\n",
        "                                                                              maximum_length,\n",
        "                                                                              num_words,\n",
        "                                                                              name = \"attn_dec_inf\")\n",
        "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                                test_decoder_function,\n",
        "                                                                                                                scope = decoding_scope)\n",
        "    return test_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJZYqbXvh9E-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Decoder RNN\n",
        "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
        "        biases = tf.zeros_initializer()\n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      None,\n",
        "                                                                      scope = decoding_scope,\n",
        "                                                                      weights_initializer = weights,\n",
        "                                                                      biases_initializer = biases)\n",
        "        training_predictions = decode_training_set(encoder_state,\n",
        "                                                   decoder_cell,\n",
        "                                                   decoder_embedded_input,\n",
        "                                                   sequence_length,\n",
        "                                                   decoding_scope,\n",
        "                                                   output_function,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        test_predictions = decode_test_set(encoder_state,\n",
        "                                           decoder_cell,\n",
        "                                           decoder_embeddings_matrix,\n",
        "                                           word2int['<SOS>'],\n",
        "                                           word2int['<EOS>'],\n",
        "                                           sequence_length - 1,\n",
        "                                           num_words,\n",
        "                                           decoding_scope,\n",
        "                                           output_function,\n",
        "                                           keep_prob,\n",
        "                                           batch_size)\n",
        "    return training_predictions, test_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SboUNY5fiCOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the seq2seq model\n",
        "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
        "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                              answers_num_words + 1,\n",
        "                                                              encoder_embedding_size,\n",
        "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
        "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
        "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
        "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
        "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
        "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
        "                                                         decoder_embeddings_matrix,\n",
        "                                                         encoder_state,\n",
        "                                                         questions_num_words,\n",
        "                                                         sequence_length,\n",
        "                                                         rnn_size,\n",
        "                                                         num_layers,\n",
        "                                                         questionswords2int,\n",
        "                                                         keep_prob,\n",
        "                                                         batch_size)\n",
        "    return training_predictions, test_predictions\n",
        " \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0foC7A-iIpR",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING THE SEQ2SEQ MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjVcb-4PiFaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKrWS8CeiM8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a session\n",
        "tf.reset_default_graph()\n",
        "session = tf.InteractiveSession()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WBDNzl7iO2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model inputs\n",
        "inputs, targets, lr, keep_prob = model_inputs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOi05l77iQ-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the sequence length\n",
        "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Chu0d2diT9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the shape of the inputs tensor\n",
        "input_shape = tf.shape(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJBmOwpziWfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the training and test predictions\n",
        "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                       targets,\n",
        "                                                       keep_prob,\n",
        "                                                       batch_size,\n",
        "                                                       sequence_length,\n",
        "                                                       len(answerswords2int),\n",
        "                                                       len(questionswords2int),\n",
        "                                                       encoding_embedding_size,\n",
        "                                                       decoding_embedding_size,\n",
        "                                                       rnn_size,\n",
        "                                                       num_layers,\n",
        "                                                       questionswords2int)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLnPdpM-iZZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up the Loss Error, the Optimizer and Gradient Clipping\n",
        "with tf.name_scope(\"optimization\"):\n",
        "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
        "                                                  targets,\n",
        "                                                  tf.ones([input_shape[0], sequence_length]))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    gradients = optimizer.compute_gradients(loss_error)\n",
        "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
        "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q3cHgiTigMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding the sequences with the <PAD> token\n",
        "def apply_padding(batch_of_sequences, word2int):\n",
        "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0jyBzhNiju7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the data into batches of questions and answers\n",
        "def split_into_batches(questions, answers, batch_size):\n",
        "    for batch_index in range(0, len(questions) // batch_size):\n",
        "        start_index = batch_index * batch_size\n",
        "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
        "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
        "        yield padded_questions_in_batch, padded_answers_in_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5DZs-sXimuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the questions and answers into training and validation sets\n",
        "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
        "training_questions = sorted_clean_questions[training_validation_split:]\n",
        "training_answers = sorted_clean_answers[training_validation_split:]\n",
        "validation_questions = sorted_clean_questions[:training_validation_split]\n",
        "validation_answers = sorted_clean_answers[:training_validation_split]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3VZ6PhXip2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "626fd5dd-db12-4dd5-b17d-95b12dc4a021"
      },
      "source": [
        "# Training\n",
        "batch_index_check_training_loss = 100\n",
        "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = []\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 100\n",
        "checkpoint = \"chatbot_weights.ckpt\"\n",
        "session.run(tf.global_variables_initializer())\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
        "        starting_time = time.time()\n",
        "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
        "                                                                                               targets: padded_answers_in_batch,\n",
        "                                                                                               lr: learning_rate,\n",
        "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                                               keep_prob: keep_probability})\n",
        "        total_training_loss_error += batch_training_loss_error\n",
        "        ending_time = time.time()\n",
        "        batch_time = ending_time - starting_time\n",
        "        if batch_index % batch_index_check_training_loss == 0:\n",
        "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_index,\n",
        "                                                                                                                                       len(training_questions) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "            total_training_loss_error = 0\n",
        "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
        "            total_validation_loss_error = 0\n",
        "            starting_time = time.time()\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
        "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
        "                                                                       targets: padded_answers_in_batch,\n",
        "                                                                       lr: learning_rate,\n",
        "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                       keep_prob: 1})\n",
        "                total_validation_loss_error += batch_validation_loss_error\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
        "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
        "            learning_rate *= learning_rate_decay\n",
        "            if learning_rate < min_learning_rate:\n",
        "                learning_rate = min_learning_rate\n",
        "            list_validation_loss_error.append(average_validation_loss_error)\n",
        "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "                print('I speak better now!!')\n",
        "                early_stopping_check = 0\n",
        "                saver = tf.train.Saver()\n",
        "                saver.save(session, checkpoint)\n",
        "            else:\n",
        "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "                early_stopping_check += 1\n",
        "                if early_stopping_check == early_stopping_stop:\n",
        "                    break\n",
        "    if early_stopping_check == early_stopping_stop:\n",
        "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "        break\n",
        "print(\"Game Over\")\n",
        " "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   1/100, Batch:    0/4120, Training Loss Error:  0.088, Training Time on 100 Batches: 184 seconds\n",
            "Epoch:   1/100, Batch:  100/4120, Training Loss Error:  2.913, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch:  200/4120, Training Loss Error:  2.356, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch:  300/4120, Training Loss Error:  2.275, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch:  400/4120, Training Loss Error:  2.193, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch:  500/4120, Training Loss Error:  2.218, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch:  600/4120, Training Loss Error:  2.186, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch:  700/4120, Training Loss Error:  2.149, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch:  800/4120, Training Loss Error:  2.161, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch:  900/4120, Training Loss Error:  2.170, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 1000/4120, Training Loss Error:  2.137, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 1100/4120, Training Loss Error:  2.139, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 1200/4120, Training Loss Error:  2.089, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 1300/4120, Training Loss Error:  2.110, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 1400/4120, Training Loss Error:  2.032, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 1500/4120, Training Loss Error:  2.058, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 1600/4120, Training Loss Error:  2.044, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 1700/4120, Training Loss Error:  2.082, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 1800/4120, Training Loss Error:  2.049, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 1900/4120, Training Loss Error:  2.026, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 2000/4120, Training Loss Error:  2.025, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.981, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   1/100, Batch: 2100/4120, Training Loss Error:  1.999, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 2200/4120, Training Loss Error:  2.108, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 2300/4120, Training Loss Error:  2.031, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 2400/4120, Training Loss Error:  1.996, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 2500/4120, Training Loss Error:  2.054, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 2600/4120, Training Loss Error:  2.021, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 2700/4120, Training Loss Error:  2.046, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 2800/4120, Training Loss Error:  2.036, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 2900/4120, Training Loss Error:  2.015, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 3000/4120, Training Loss Error:  1.989, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 3100/4120, Training Loss Error:  2.076, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 3200/4120, Training Loss Error:  1.944, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 3300/4120, Training Loss Error:  1.973, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 3400/4120, Training Loss Error:  2.008, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 3500/4120, Training Loss Error:  1.965, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 3600/4120, Training Loss Error:  1.962, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 3700/4120, Training Loss Error:  1.994, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 3800/4120, Training Loss Error:  1.934, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 3900/4120, Training Loss Error:  1.997, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 4000/4120, Training Loss Error:  1.978, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 4100/4120, Training Loss Error:  1.984, Training Time on 100 Batches: 34 seconds\n",
            "Validation Loss Error:  1.996, Batch Validation Time: 53 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   2/100, Batch:    0/4120, Training Loss Error:  0.382, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   2/100, Batch:  100/4120, Training Loss Error:  1.923, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   2/100, Batch:  200/4120, Training Loss Error:  1.924, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   2/100, Batch:  300/4120, Training Loss Error:  1.899, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   2/100, Batch:  400/4120, Training Loss Error:  1.857, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   2/100, Batch:  500/4120, Training Loss Error:  1.900, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch:  600/4120, Training Loss Error:  1.882, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   2/100, Batch:  700/4120, Training Loss Error:  1.864, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch:  800/4120, Training Loss Error:  1.891, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch:  900/4120, Training Loss Error:  1.918, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch: 1000/4120, Training Loss Error:  1.896, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch: 1100/4120, Training Loss Error:  1.922, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   2/100, Batch: 1200/4120, Training Loss Error:  1.886, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch: 1300/4120, Training Loss Error:  1.911, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   2/100, Batch: 1400/4120, Training Loss Error:  1.850, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   2/100, Batch: 1500/4120, Training Loss Error:  1.887, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   2/100, Batch: 1600/4120, Training Loss Error:  1.877, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch: 1700/4120, Training Loss Error:  1.912, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   2/100, Batch: 1800/4120, Training Loss Error:  1.893, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   2/100, Batch: 1900/4120, Training Loss Error:  1.875, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   2/100, Batch: 2000/4120, Training Loss Error:  1.879, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.843, Batch Validation Time: 53 seconds\n",
            "I speak better now!!\n",
            "Epoch:   2/100, Batch: 2100/4120, Training Loss Error:  1.863, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch: 2200/4120, Training Loss Error:  1.972, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch: 2300/4120, Training Loss Error:  1.900, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   2/100, Batch: 2400/4120, Training Loss Error:  1.874, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   2/100, Batch: 2500/4120, Training Loss Error:  1.927, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   2/100, Batch: 2600/4120, Training Loss Error:  1.906, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   2/100, Batch: 2700/4120, Training Loss Error:  1.929, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   2/100, Batch: 2800/4120, Training Loss Error:  1.920, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   2/100, Batch: 2900/4120, Training Loss Error:  1.907, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   2/100, Batch: 3000/4120, Training Loss Error:  1.883, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   2/100, Batch: 3100/4120, Training Loss Error:  1.971, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   2/100, Batch: 3200/4120, Training Loss Error:  1.847, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   2/100, Batch: 3300/4120, Training Loss Error:  1.873, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   2/100, Batch: 3400/4120, Training Loss Error:  1.908, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   2/100, Batch: 3500/4120, Training Loss Error:  1.871, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   2/100, Batch: 3600/4120, Training Loss Error:  1.873, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   2/100, Batch: 3700/4120, Training Loss Error:  1.905, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   2/100, Batch: 3800/4120, Training Loss Error:  1.851, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   2/100, Batch: 3900/4120, Training Loss Error:  1.909, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   2/100, Batch: 4000/4120, Training Loss Error:  1.893, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   2/100, Batch: 4100/4120, Training Loss Error:  1.900, Training Time on 100 Batches: 33 seconds\n",
            "Validation Loss Error:  1.832, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   3/100, Batch:    0/4120, Training Loss Error:  0.366, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   3/100, Batch:  100/4120, Training Loss Error:  1.848, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   3/100, Batch:  200/4120, Training Loss Error:  1.853, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   3/100, Batch:  300/4120, Training Loss Error:  1.832, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   3/100, Batch:  400/4120, Training Loss Error:  1.788, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   3/100, Batch:  500/4120, Training Loss Error:  1.832, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   3/100, Batch:  600/4120, Training Loss Error:  1.815, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   3/100, Batch:  700/4120, Training Loss Error:  1.798, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   3/100, Batch:  800/4120, Training Loss Error:  1.830, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   3/100, Batch:  900/4120, Training Loss Error:  1.848, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 1000/4120, Training Loss Error:  1.834, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   3/100, Batch: 1100/4120, Training Loss Error:  1.861, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 1200/4120, Training Loss Error:  1.827, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   3/100, Batch: 1300/4120, Training Loss Error:  1.848, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 1400/4120, Training Loss Error:  1.794, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 1500/4120, Training Loss Error:  1.829, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 1600/4120, Training Loss Error:  1.819, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 1700/4120, Training Loss Error:  1.855, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   3/100, Batch: 1800/4120, Training Loss Error:  1.841, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   3/100, Batch: 1900/4120, Training Loss Error:  1.819, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   3/100, Batch: 2000/4120, Training Loss Error:  1.825, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.799, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   3/100, Batch: 2100/4120, Training Loss Error:  1.807, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   3/100, Batch: 2200/4120, Training Loss Error:  1.917, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   3/100, Batch: 2300/4120, Training Loss Error:  1.846, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   3/100, Batch: 2400/4120, Training Loss Error:  1.823, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   3/100, Batch: 2500/4120, Training Loss Error:  1.873, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 2600/4120, Training Loss Error:  1.859, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 2700/4120, Training Loss Error:  1.876, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   3/100, Batch: 2800/4120, Training Loss Error:  1.870, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   3/100, Batch: 2900/4120, Training Loss Error:  1.856, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   3/100, Batch: 3000/4120, Training Loss Error:  1.838, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   3/100, Batch: 3100/4120, Training Loss Error:  1.921, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   3/100, Batch: 3200/4120, Training Loss Error:  1.798, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   3/100, Batch: 3300/4120, Training Loss Error:  1.824, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   3/100, Batch: 3400/4120, Training Loss Error:  1.861, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   3/100, Batch: 3500/4120, Training Loss Error:  1.825, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   3/100, Batch: 3600/4120, Training Loss Error:  1.825, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   3/100, Batch: 3700/4120, Training Loss Error:  1.859, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   3/100, Batch: 3800/4120, Training Loss Error:  1.809, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   3/100, Batch: 3900/4120, Training Loss Error:  1.865, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   3/100, Batch: 4000/4120, Training Loss Error:  1.845, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   3/100, Batch: 4100/4120, Training Loss Error:  1.858, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.795, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   4/100, Batch:    0/4120, Training Loss Error:  0.357, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   4/100, Batch:  100/4120, Training Loss Error:  1.808, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   4/100, Batch:  200/4120, Training Loss Error:  1.813, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   4/100, Batch:  300/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   4/100, Batch:  400/4120, Training Loss Error:  1.754, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   4/100, Batch:  500/4120, Training Loss Error:  1.795, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch:  600/4120, Training Loss Error:  1.777, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   4/100, Batch:  700/4120, Training Loss Error:  1.762, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch:  800/4120, Training Loss Error:  1.792, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   4/100, Batch:  900/4120, Training Loss Error:  1.811, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 1000/4120, Training Loss Error:  1.799, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 1100/4120, Training Loss Error:  1.819, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   4/100, Batch: 1200/4120, Training Loss Error:  1.792, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 1300/4120, Training Loss Error:  1.812, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 1400/4120, Training Loss Error:  1.760, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 1500/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   4/100, Batch: 1600/4120, Training Loss Error:  1.784, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   4/100, Batch: 1700/4120, Training Loss Error:  1.822, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   4/100, Batch: 1800/4120, Training Loss Error:  1.805, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   4/100, Batch: 1900/4120, Training Loss Error:  1.786, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 2000/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.776, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   4/100, Batch: 2100/4120, Training Loss Error:  1.776, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   4/100, Batch: 2200/4120, Training Loss Error:  1.882, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 2300/4120, Training Loss Error:  1.813, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   4/100, Batch: 2400/4120, Training Loss Error:  1.792, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   4/100, Batch: 2500/4120, Training Loss Error:  1.838, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   4/100, Batch: 2600/4120, Training Loss Error:  1.826, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   4/100, Batch: 2700/4120, Training Loss Error:  1.845, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   4/100, Batch: 2800/4120, Training Loss Error:  1.839, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   4/100, Batch: 2900/4120, Training Loss Error:  1.824, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   4/100, Batch: 3000/4120, Training Loss Error:  1.805, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   4/100, Batch: 3100/4120, Training Loss Error:  1.890, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   4/100, Batch: 3200/4120, Training Loss Error:  1.771, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   4/100, Batch: 3300/4120, Training Loss Error:  1.797, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   4/100, Batch: 3400/4120, Training Loss Error:  1.831, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   4/100, Batch: 3500/4120, Training Loss Error:  1.796, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   4/100, Batch: 3600/4120, Training Loss Error:  1.796, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   4/100, Batch: 3700/4120, Training Loss Error:  1.828, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   4/100, Batch: 3800/4120, Training Loss Error:  1.778, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   4/100, Batch: 3900/4120, Training Loss Error:  1.836, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   4/100, Batch: 4000/4120, Training Loss Error:  1.817, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   4/100, Batch: 4100/4120, Training Loss Error:  1.825, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.773, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   5/100, Batch:    0/4120, Training Loss Error:  0.352, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   5/100, Batch:  100/4120, Training Loss Error:  1.779, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   5/100, Batch:  200/4120, Training Loss Error:  1.782, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   5/100, Batch:  300/4120, Training Loss Error:  1.769, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   5/100, Batch:  400/4120, Training Loss Error:  1.725, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   5/100, Batch:  500/4120, Training Loss Error:  1.769, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   5/100, Batch:  600/4120, Training Loss Error:  1.751, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   5/100, Batch:  700/4120, Training Loss Error:  1.737, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   5/100, Batch:  800/4120, Training Loss Error:  1.764, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   5/100, Batch:  900/4120, Training Loss Error:  1.782, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   5/100, Batch: 1000/4120, Training Loss Error:  1.773, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   5/100, Batch: 1100/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   5/100, Batch: 1200/4120, Training Loss Error:  1.765, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   5/100, Batch: 1300/4120, Training Loss Error:  1.783, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   5/100, Batch: 1400/4120, Training Loss Error:  1.738, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   5/100, Batch: 1500/4120, Training Loss Error:  1.770, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   5/100, Batch: 1600/4120, Training Loss Error:  1.759, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   5/100, Batch: 1700/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   5/100, Batch: 1800/4120, Training Loss Error:  1.782, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   5/100, Batch: 1900/4120, Training Loss Error:  1.759, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   5/100, Batch: 2000/4120, Training Loss Error:  1.766, Training Time on 100 Batches: 24 seconds\n",
            "Validation Loss Error:  1.761, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   5/100, Batch: 2100/4120, Training Loss Error:  1.752, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   5/100, Batch: 2200/4120, Training Loss Error:  1.855, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   5/100, Batch: 2300/4120, Training Loss Error:  1.789, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   5/100, Batch: 2400/4120, Training Loss Error:  1.767, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   5/100, Batch: 2500/4120, Training Loss Error:  1.811, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   5/100, Batch: 2600/4120, Training Loss Error:  1.800, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   5/100, Batch: 2700/4120, Training Loss Error:  1.820, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   5/100, Batch: 2800/4120, Training Loss Error:  1.812, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   5/100, Batch: 2900/4120, Training Loss Error:  1.800, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   5/100, Batch: 3000/4120, Training Loss Error:  1.783, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   5/100, Batch: 3100/4120, Training Loss Error:  1.863, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   5/100, Batch: 3200/4120, Training Loss Error:  1.748, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   5/100, Batch: 3300/4120, Training Loss Error:  1.772, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   5/100, Batch: 3400/4120, Training Loss Error:  1.809, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   5/100, Batch: 3500/4120, Training Loss Error:  1.774, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   5/100, Batch: 3600/4120, Training Loss Error:  1.772, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   5/100, Batch: 3700/4120, Training Loss Error:  1.805, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   5/100, Batch: 3800/4120, Training Loss Error:  1.753, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   5/100, Batch: 3900/4120, Training Loss Error:  1.810, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   5/100, Batch: 4000/4120, Training Loss Error:  1.796, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   5/100, Batch: 4100/4120, Training Loss Error:  1.805, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.761, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   6/100, Batch:    0/4120, Training Loss Error:  0.346, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   6/100, Batch:  100/4120, Training Loss Error:  1.757, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   6/100, Batch:  200/4120, Training Loss Error:  1.757, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   6/100, Batch:  300/4120, Training Loss Error:  1.747, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   6/100, Batch:  400/4120, Training Loss Error:  1.705, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   6/100, Batch:  500/4120, Training Loss Error:  1.752, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   6/100, Batch:  600/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   6/100, Batch:  700/4120, Training Loss Error:  1.718, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   6/100, Batch:  800/4120, Training Loss Error:  1.746, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   6/100, Batch:  900/4120, Training Loss Error:  1.761, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   6/100, Batch: 1000/4120, Training Loss Error:  1.751, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   6/100, Batch: 1100/4120, Training Loss Error:  1.771, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   6/100, Batch: 1200/4120, Training Loss Error:  1.747, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   6/100, Batch: 1300/4120, Training Loss Error:  1.765, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   6/100, Batch: 1400/4120, Training Loss Error:  1.716, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   6/100, Batch: 1500/4120, Training Loss Error:  1.751, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   6/100, Batch: 1600/4120, Training Loss Error:  1.740, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   6/100, Batch: 1700/4120, Training Loss Error:  1.771, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   6/100, Batch: 1800/4120, Training Loss Error:  1.764, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   6/100, Batch: 1900/4120, Training Loss Error:  1.740, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   6/100, Batch: 2000/4120, Training Loss Error:  1.747, Training Time on 100 Batches: 26 seconds\n",
            "Validation Loss Error:  1.751, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   6/100, Batch: 2100/4120, Training Loss Error:  1.732, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   6/100, Batch: 2200/4120, Training Loss Error:  1.834, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   6/100, Batch: 2300/4120, Training Loss Error:  1.770, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   6/100, Batch: 2400/4120, Training Loss Error:  1.749, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   6/100, Batch: 2500/4120, Training Loss Error:  1.791, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   6/100, Batch: 2600/4120, Training Loss Error:  1.782, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   6/100, Batch: 2700/4120, Training Loss Error:  1.804, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   6/100, Batch: 2800/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   6/100, Batch: 2900/4120, Training Loss Error:  1.778, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   6/100, Batch: 3000/4120, Training Loss Error:  1.762, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   6/100, Batch: 3100/4120, Training Loss Error:  1.845, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   6/100, Batch: 3200/4120, Training Loss Error:  1.727, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   6/100, Batch: 3300/4120, Training Loss Error:  1.752, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   6/100, Batch: 3400/4120, Training Loss Error:  1.791, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   6/100, Batch: 3500/4120, Training Loss Error:  1.753, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   6/100, Batch: 3600/4120, Training Loss Error:  1.752, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   6/100, Batch: 3700/4120, Training Loss Error:  1.787, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   6/100, Batch: 3800/4120, Training Loss Error:  1.736, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   6/100, Batch: 3900/4120, Training Loss Error:  1.792, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   6/100, Batch: 4000/4120, Training Loss Error:  1.776, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   6/100, Batch: 4100/4120, Training Loss Error:  1.782, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.753, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   7/100, Batch:    0/4120, Training Loss Error:  0.342, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   7/100, Batch:  100/4120, Training Loss Error:  1.740, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   7/100, Batch:  200/4120, Training Loss Error:  1.742, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   7/100, Batch:  300/4120, Training Loss Error:  1.729, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   7/100, Batch:  400/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch:  500/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   7/100, Batch:  600/4120, Training Loss Error:  1.709, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   7/100, Batch:  700/4120, Training Loss Error:  1.700, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch:  800/4120, Training Loss Error:  1.728, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   7/100, Batch:  900/4120, Training Loss Error:  1.745, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 1000/4120, Training Loss Error:  1.733, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   7/100, Batch: 1100/4120, Training Loss Error:  1.755, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 1200/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 1300/4120, Training Loss Error:  1.746, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 1400/4120, Training Loss Error:  1.703, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   7/100, Batch: 1500/4120, Training Loss Error:  1.733, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   7/100, Batch: 1600/4120, Training Loss Error:  1.724, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 1700/4120, Training Loss Error:  1.757, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   7/100, Batch: 1800/4120, Training Loss Error:  1.745, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   7/100, Batch: 1900/4120, Training Loss Error:  1.722, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   7/100, Batch: 2000/4120, Training Loss Error:  1.732, Training Time on 100 Batches: 24 seconds\n",
            "Validation Loss Error:  1.746, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   7/100, Batch: 2100/4120, Training Loss Error:  1.716, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 2200/4120, Training Loss Error:  1.815, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   7/100, Batch: 2300/4120, Training Loss Error:  1.753, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   7/100, Batch: 2400/4120, Training Loss Error:  1.735, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   7/100, Batch: 2500/4120, Training Loss Error:  1.774, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 2600/4120, Training Loss Error:  1.764, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   7/100, Batch: 2700/4120, Training Loss Error:  1.784, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   7/100, Batch: 2800/4120, Training Loss Error:  1.775, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   7/100, Batch: 2900/4120, Training Loss Error:  1.758, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   7/100, Batch: 3000/4120, Training Loss Error:  1.745, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   7/100, Batch: 3100/4120, Training Loss Error:  1.827, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   7/100, Batch: 3200/4120, Training Loss Error:  1.709, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   7/100, Batch: 3300/4120, Training Loss Error:  1.737, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   7/100, Batch: 3400/4120, Training Loss Error:  1.770, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   7/100, Batch: 3500/4120, Training Loss Error:  1.739, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   7/100, Batch: 3600/4120, Training Loss Error:  1.736, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   7/100, Batch: 3700/4120, Training Loss Error:  1.772, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   7/100, Batch: 3800/4120, Training Loss Error:  1.718, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   7/100, Batch: 3900/4120, Training Loss Error:  1.774, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   7/100, Batch: 4000/4120, Training Loss Error:  1.756, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   7/100, Batch: 4100/4120, Training Loss Error:  1.767, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.756, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   8/100, Batch:    0/4120, Training Loss Error:  0.339, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   8/100, Batch:  100/4120, Training Loss Error:  1.724, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   8/100, Batch:  200/4120, Training Loss Error:  1.729, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   8/100, Batch:  300/4120, Training Loss Error:  1.712, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   8/100, Batch:  400/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   8/100, Batch:  500/4120, Training Loss Error:  1.716, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   8/100, Batch:  600/4120, Training Loss Error:  1.693, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   8/100, Batch:  700/4120, Training Loss Error:  1.683, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch:  800/4120, Training Loss Error:  1.711, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   8/100, Batch:  900/4120, Training Loss Error:  1.727, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch: 1000/4120, Training Loss Error:  1.719, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   8/100, Batch: 1100/4120, Training Loss Error:  1.739, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch: 1200/4120, Training Loss Error:  1.712, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   8/100, Batch: 1300/4120, Training Loss Error:  1.732, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch: 1400/4120, Training Loss Error:  1.688, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch: 1500/4120, Training Loss Error:  1.717, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   8/100, Batch: 1600/4120, Training Loss Error:  1.708, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   8/100, Batch: 1700/4120, Training Loss Error:  1.739, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   8/100, Batch: 1800/4120, Training Loss Error:  1.733, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   8/100, Batch: 1900/4120, Training Loss Error:  1.705, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch: 2000/4120, Training Loss Error:  1.716, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.744, Batch Validation Time: 54 seconds\n",
            "I speak better now!!\n",
            "Epoch:   8/100, Batch: 2100/4120, Training Loss Error:  1.698, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch: 2200/4120, Training Loss Error:  1.800, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   8/100, Batch: 2300/4120, Training Loss Error:  1.741, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   8/100, Batch: 2400/4120, Training Loss Error:  1.720, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   8/100, Batch: 2500/4120, Training Loss Error:  1.758, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   8/100, Batch: 2600/4120, Training Loss Error:  1.748, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   8/100, Batch: 2700/4120, Training Loss Error:  1.769, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   8/100, Batch: 2800/4120, Training Loss Error:  1.759, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   8/100, Batch: 2900/4120, Training Loss Error:  1.746, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   8/100, Batch: 3000/4120, Training Loss Error:  1.732, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   8/100, Batch: 3100/4120, Training Loss Error:  1.810, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   8/100, Batch: 3200/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   8/100, Batch: 3300/4120, Training Loss Error:  1.719, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   8/100, Batch: 3400/4120, Training Loss Error:  1.757, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   8/100, Batch: 3500/4120, Training Loss Error:  1.721, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   8/100, Batch: 3600/4120, Training Loss Error:  1.721, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   8/100, Batch: 3700/4120, Training Loss Error:  1.756, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   8/100, Batch: 3800/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   8/100, Batch: 3900/4120, Training Loss Error:  1.756, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   8/100, Batch: 4000/4120, Training Loss Error:  1.743, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   8/100, Batch: 4100/4120, Training Loss Error:  1.751, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.750, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   9/100, Batch:    0/4120, Training Loss Error:  0.336, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   9/100, Batch:  100/4120, Training Loss Error:  1.712, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   9/100, Batch:  200/4120, Training Loss Error:  1.711, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   9/100, Batch:  300/4120, Training Loss Error:  1.697, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch:  400/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   9/100, Batch:  500/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch:  600/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   9/100, Batch:  700/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch:  800/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch:  900/4120, Training Loss Error:  1.713, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch: 1000/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   9/100, Batch: 1100/4120, Training Loss Error:  1.724, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch: 1200/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch: 1300/4120, Training Loss Error:  1.717, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch: 1400/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch: 1500/4120, Training Loss Error:  1.706, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch: 1600/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch: 1700/4120, Training Loss Error:  1.729, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   9/100, Batch: 1800/4120, Training Loss Error:  1.718, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   9/100, Batch: 1900/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch: 2000/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.745, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   9/100, Batch: 2100/4120, Training Loss Error:  1.685, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch: 2200/4120, Training Loss Error:  1.786, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch: 2300/4120, Training Loss Error:  1.728, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   9/100, Batch: 2400/4120, Training Loss Error:  1.704, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   9/100, Batch: 2500/4120, Training Loss Error:  1.741, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch: 2600/4120, Training Loss Error:  1.734, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   9/100, Batch: 2700/4120, Training Loss Error:  1.753, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   9/100, Batch: 2800/4120, Training Loss Error:  1.745, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   9/100, Batch: 2900/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   9/100, Batch: 3000/4120, Training Loss Error:  1.715, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   9/100, Batch: 3100/4120, Training Loss Error:  1.793, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   9/100, Batch: 3200/4120, Training Loss Error:  1.681, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   9/100, Batch: 3300/4120, Training Loss Error:  1.706, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   9/100, Batch: 3400/4120, Training Loss Error:  1.741, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   9/100, Batch: 3500/4120, Training Loss Error:  1.709, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   9/100, Batch: 3600/4120, Training Loss Error:  1.707, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   9/100, Batch: 3700/4120, Training Loss Error:  1.743, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   9/100, Batch: 3800/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   9/100, Batch: 3900/4120, Training Loss Error:  1.743, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   9/100, Batch: 4000/4120, Training Loss Error:  1.728, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   9/100, Batch: 4100/4120, Training Loss Error:  1.736, Training Time on 100 Batches: 33 seconds\n",
            "Validation Loss Error:  1.750, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  10/100, Batch:    0/4120, Training Loss Error:  0.334, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  10/100, Batch:  100/4120, Training Loss Error:  1.699, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch:  200/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:  10/100, Batch:  300/4120, Training Loss Error:  1.685, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  10/100, Batch:  400/4120, Training Loss Error:  1.644, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  10/100, Batch:  500/4120, Training Loss Error:  1.688, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch:  600/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  10/100, Batch:  700/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  10/100, Batch:  800/4120, Training Loss Error:  1.683, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch:  900/4120, Training Loss Error:  1.699, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  10/100, Batch: 1000/4120, Training Loss Error:  1.695, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  10/100, Batch: 1100/4120, Training Loss Error:  1.709, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  10/100, Batch: 1200/4120, Training Loss Error:  1.687, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch: 1300/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch: 1400/4120, Training Loss Error:  1.657, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch: 1500/4120, Training Loss Error:  1.690, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  10/100, Batch: 1600/4120, Training Loss Error:  1.681, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch: 1700/4120, Training Loss Error:  1.713, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  10/100, Batch: 1800/4120, Training Loss Error:  1.707, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  10/100, Batch: 1900/4120, Training Loss Error:  1.678, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  10/100, Batch: 2000/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.747, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  10/100, Batch: 2100/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch: 2200/4120, Training Loss Error:  1.768, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  10/100, Batch: 2300/4120, Training Loss Error:  1.711, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  10/100, Batch: 2400/4120, Training Loss Error:  1.693, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  10/100, Batch: 2500/4120, Training Loss Error:  1.727, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch: 2600/4120, Training Loss Error:  1.720, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  10/100, Batch: 2700/4120, Training Loss Error:  1.741, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  10/100, Batch: 2800/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  10/100, Batch: 2900/4120, Training Loss Error:  1.717, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  10/100, Batch: 3000/4120, Training Loss Error:  1.704, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  10/100, Batch: 3100/4120, Training Loss Error:  1.776, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  10/100, Batch: 3200/4120, Training Loss Error:  1.666, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  10/100, Batch: 3300/4120, Training Loss Error:  1.693, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  10/100, Batch: 3400/4120, Training Loss Error:  1.726, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  10/100, Batch: 3500/4120, Training Loss Error:  1.698, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  10/100, Batch: 3600/4120, Training Loss Error:  1.695, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  10/100, Batch: 3700/4120, Training Loss Error:  1.727, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  10/100, Batch: 3800/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:  10/100, Batch: 3900/4120, Training Loss Error:  1.730, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  10/100, Batch: 4000/4120, Training Loss Error:  1.714, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  10/100, Batch: 4100/4120, Training Loss Error:  1.722, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.755, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  11/100, Batch:    0/4120, Training Loss Error:  0.331, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  11/100, Batch:  100/4120, Training Loss Error:  1.687, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch:  200/4120, Training Loss Error:  1.686, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:  11/100, Batch:  300/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  11/100, Batch:  400/4120, Training Loss Error:  1.630, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  11/100, Batch:  500/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch:  600/4120, Training Loss Error:  1.653, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  11/100, Batch:  700/4120, Training Loss Error:  1.647, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch:  800/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch:  900/4120, Training Loss Error:  1.684, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch: 1000/4120, Training Loss Error:  1.678, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch: 1100/4120, Training Loss Error:  1.695, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  11/100, Batch: 1200/4120, Training Loss Error:  1.672, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch: 1300/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  11/100, Batch: 1400/4120, Training Loss Error:  1.647, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  11/100, Batch: 1500/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  11/100, Batch: 1600/4120, Training Loss Error:  1.669, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch: 1700/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  11/100, Batch: 1800/4120, Training Loss Error:  1.696, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  11/100, Batch: 1900/4120, Training Loss Error:  1.663, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  11/100, Batch: 2000/4120, Training Loss Error:  1.673, Training Time on 100 Batches: 24 seconds\n",
            "Validation Loss Error:  1.745, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  11/100, Batch: 2100/4120, Training Loss Error:  1.661, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch: 2200/4120, Training Loss Error:  1.754, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  11/100, Batch: 2300/4120, Training Loss Error:  1.701, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  11/100, Batch: 2400/4120, Training Loss Error:  1.680, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  11/100, Batch: 2500/4120, Training Loss Error:  1.715, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch: 2600/4120, Training Loss Error:  1.707, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  11/100, Batch: 2700/4120, Training Loss Error:  1.728, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  11/100, Batch: 2800/4120, Training Loss Error:  1.718, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  11/100, Batch: 2900/4120, Training Loss Error:  1.703, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  11/100, Batch: 3000/4120, Training Loss Error:  1.691, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  11/100, Batch: 3100/4120, Training Loss Error:  1.764, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  11/100, Batch: 3200/4120, Training Loss Error:  1.655, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  11/100, Batch: 3300/4120, Training Loss Error:  1.682, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  11/100, Batch: 3400/4120, Training Loss Error:  1.714, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  11/100, Batch: 3500/4120, Training Loss Error:  1.682, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  11/100, Batch: 3600/4120, Training Loss Error:  1.683, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  11/100, Batch: 3700/4120, Training Loss Error:  1.714, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  11/100, Batch: 3800/4120, Training Loss Error:  1.664, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:  11/100, Batch: 3900/4120, Training Loss Error:  1.716, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  11/100, Batch: 4000/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  11/100, Batch: 4100/4120, Training Loss Error:  1.709, Training Time on 100 Batches: 31 seconds\n",
            "Validation Loss Error:  1.758, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  12/100, Batch:    0/4120, Training Loss Error:  0.328, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  12/100, Batch:  100/4120, Training Loss Error:  1.677, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  12/100, Batch:  200/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:  12/100, Batch:  300/4120, Training Loss Error:  1.662, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  12/100, Batch:  400/4120, Training Loss Error:  1.617, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  12/100, Batch:  500/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch:  600/4120, Training Loss Error:  1.639, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  12/100, Batch:  700/4120, Training Loss Error:  1.634, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  12/100, Batch:  800/4120, Training Loss Error:  1.661, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch:  900/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch: 1000/4120, Training Loss Error:  1.667, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  12/100, Batch: 1100/4120, Training Loss Error:  1.685, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  12/100, Batch: 1200/4120, Training Loss Error:  1.658, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch: 1300/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch: 1400/4120, Training Loss Error:  1.636, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  12/100, Batch: 1500/4120, Training Loss Error:  1.665, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  12/100, Batch: 1600/4120, Training Loss Error:  1.655, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  12/100, Batch: 1700/4120, Training Loss Error:  1.682, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  12/100, Batch: 1800/4120, Training Loss Error:  1.683, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  12/100, Batch: 1900/4120, Training Loss Error:  1.650, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  12/100, Batch: 2000/4120, Training Loss Error:  1.664, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.746, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  12/100, Batch: 2100/4120, Training Loss Error:  1.647, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch: 2200/4120, Training Loss Error:  1.742, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch: 2300/4120, Training Loss Error:  1.690, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  12/100, Batch: 2400/4120, Training Loss Error:  1.669, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  12/100, Batch: 2500/4120, Training Loss Error:  1.700, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch: 2600/4120, Training Loss Error:  1.695, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  12/100, Batch: 2700/4120, Training Loss Error:  1.715, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  12/100, Batch: 2800/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  12/100, Batch: 2900/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  12/100, Batch: 3000/4120, Training Loss Error:  1.681, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  12/100, Batch: 3100/4120, Training Loss Error:  1.752, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  12/100, Batch: 3200/4120, Training Loss Error:  1.641, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  12/100, Batch: 3300/4120, Training Loss Error:  1.667, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  12/100, Batch: 3400/4120, Training Loss Error:  1.702, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  12/100, Batch: 3500/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  12/100, Batch: 3600/4120, Training Loss Error:  1.669, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  12/100, Batch: 3700/4120, Training Loss Error:  1.705, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  12/100, Batch: 3800/4120, Training Loss Error:  1.650, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:  12/100, Batch: 3900/4120, Training Loss Error:  1.705, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  12/100, Batch: 4000/4120, Training Loss Error:  1.693, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  12/100, Batch: 4100/4120, Training Loss Error:  1.692, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.761, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  13/100, Batch:    0/4120, Training Loss Error:  0.326, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  13/100, Batch:  100/4120, Training Loss Error:  1.662, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch:  200/4120, Training Loss Error:  1.660, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:  13/100, Batch:  300/4120, Training Loss Error:  1.650, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch:  400/4120, Training Loss Error:  1.606, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:  13/100, Batch:  500/4120, Training Loss Error:  1.652, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch:  600/4120, Training Loss Error:  1.627, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  13/100, Batch:  700/4120, Training Loss Error:  1.624, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  13/100, Batch:  800/4120, Training Loss Error:  1.647, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  13/100, Batch:  900/4120, Training Loss Error:  1.658, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  13/100, Batch: 1000/4120, Training Loss Error:  1.656, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  13/100, Batch: 1100/4120, Training Loss Error:  1.671, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  13/100, Batch: 1200/4120, Training Loss Error:  1.648, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch: 1300/4120, Training Loss Error:  1.668, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  13/100, Batch: 1400/4120, Training Loss Error:  1.622, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  13/100, Batch: 1500/4120, Training Loss Error:  1.654, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  13/100, Batch: 1600/4120, Training Loss Error:  1.644, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch: 1700/4120, Training Loss Error:  1.674, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  13/100, Batch: 1800/4120, Training Loss Error:  1.670, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  13/100, Batch: 1900/4120, Training Loss Error:  1.641, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  13/100, Batch: 2000/4120, Training Loss Error:  1.651, Training Time on 100 Batches: 25 seconds\n",
            "Validation Loss Error:  1.751, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  13/100, Batch: 2100/4120, Training Loss Error:  1.634, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch: 2200/4120, Training Loss Error:  1.725, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch: 2300/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  13/100, Batch: 2400/4120, Training Loss Error:  1.655, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  13/100, Batch: 2500/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  13/100, Batch: 2600/4120, Training Loss Error:  1.684, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  13/100, Batch: 2700/4120, Training Loss Error:  1.703, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  13/100, Batch: 2800/4120, Training Loss Error:  1.689, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:  13/100, Batch: 2900/4120, Training Loss Error:  1.676, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  13/100, Batch: 3000/4120, Training Loss Error:  1.667, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  13/100, Batch: 3100/4120, Training Loss Error:  1.739, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  13/100, Batch: 3200/4120, Training Loss Error:  1.630, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  13/100, Batch: 3300/4120, Training Loss Error:  1.655, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  13/100, Batch: 3400/4120, Training Loss Error:  1.688, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:  13/100, Batch: 3500/4120, Training Loss Error:  1.658, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  13/100, Batch: 3600/4120, Training Loss Error:  1.659, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:  13/100, Batch: 3700/4120, Training Loss Error:  1.694, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:  13/100, Batch: 3800/4120, Training Loss Error:  1.639, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  13/100, Batch: 3900/4120, Training Loss Error:  1.692, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:  13/100, Batch: 4000/4120, Training Loss Error:  1.675, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:  13/100, Batch: 4100/4120, Training Loss Error:  1.680, Training Time on 100 Batches: 32 seconds\n",
            "Validation Loss Error:  1.766, Batch Validation Time: 54 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:  14/100, Batch:    0/4120, Training Loss Error:  0.325, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  14/100, Batch:  100/4120, Training Loss Error:  1.648, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  14/100, Batch:  200/4120, Training Loss Error:  1.645, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:  14/100, Batch:  300/4120, Training Loss Error:  1.635, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  14/100, Batch:  400/4120, Training Loss Error:  1.597, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:  14/100, Batch:  500/4120, Training Loss Error:  1.638, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:  14/100, Batch:  600/4120, Training Loss Error:  1.614, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:  14/100, Batch:  700/4120, Training Loss Error:  1.612, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:  14/100, Batch:  800/4120, Training Loss Error:  1.637, Training Time on 100 Batches: 22 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-9a9bb7dce870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                                                                \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                                                \u001b[0msequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded_answers_in_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                                                                keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_training_loss_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_training_loss_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mending_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7JrLv_-i8qZ",
        "colab_type": "text"
      },
      "source": [
        "# TESTING THE SEQ2SEQ MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwHEPi1TiySr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the weights and Running the session\n",
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(session, checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afy32H_6jDkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the questions from strings to lists of encoding integers\n",
        "def convert_string2int(question, word2int):\n",
        "    question = clean_text(question)\n",
        "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zawmJZQFjHXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "8e5c8dad-6aec-4f62-97ba-d29a12fb55ca"
      },
      "source": [
        "# Setting up the chat\n",
        "while(True):\n",
        "    question = input(\"You: \")\n",
        "    if question == 'Goodbye':\n",
        "        break\n",
        "    question = convert_string2int(question, questionswords2int)\n",
        "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
        "    fake_batch = np.zeros((batch_size, 25))\n",
        "    fake_batch[0] = question\n",
        "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
        "    answer = ''\n",
        "    for i in np.argmax(predicted_answer, 1):\n",
        "        if answersints2word[i] == 'i':\n",
        "            token = ' I'\n",
        "        elif answersints2word[i] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif answersints2word[i] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + answersints2word[i]\n",
        "        answer += token\n",
        "        if token == '.':\n",
        "            break\n",
        "    print('ChatBot: ' + answer)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You: hi\n",
            "ChatBot:  I am not sure.\n",
            "You: How r you?\n",
            "ChatBot:  I am not sure.\n",
            "You: what are you doing?\n",
            "ChatBot:  I am not sure.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-63b29181dc17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Goodbye'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_string2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestionswords2int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnklwehPdEgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}